# 목차
- 1파트 쿠버네티스 빠르게 훑어보기 (6장)
    - 6.1 쿠버네티스는 어떻게 애플리케이션을 스케일링하는가
    - 6.2 디플로이먼트와 레플리카셋을 이용한 부하 스케일링
    - 6.3 데몬셋을 이용한 스케일링으로 고가용성 확보하기
    - 6.4 쿠버네티스의 객체 간 오너십
    - 6.5 연습 문제
    
# 서론
앱 스케일링 기본은 파드를 늘리는 것이다.  
K8s는 동일한 앱이 돌아가는 파드를 레플리카(replica)라 한다.  
그리고 클러스터 내의 여러 노드에 분산된다.  
이런 간단한 방식으로 더 많은 요청을 처리하고 고가용성을 확보한다.  
  
# 6.1 쿠버네티스는 어떻게 애플리케이션을 스케일링하는가
K8s에서 파드는 컴퓨팅 단위다.  
컨트롤러 리소스를 통해 파드를 관리한다.  
컨트롤러 리소스는 파드 템플릿을 포함한다. 파드가 재생성되고 대체될 때 이 템플릿을 사용한다.  
파드를 직접 관리하는 역할은 디플로이먼트가 아니라 '레플리카셋'의 몫이다.  
- 디플로이먼트 (레플리카셋 관리)
  - 레플리카셋 (파드 관리 - 대체 파드 생성 등)
    - 파드 (컨테이너 관리 - 대체 컨테이너 생성 등)
      - 컨테이너
      - 컨테이너
    - 파드
  
대부분의 경우 앱 정의는 디플로이먼트 형태로 기술한다.  
디플로이먼트 없이 레플리카셋을 바로 만들 수도 있다.

- 예제 6-1
  - 레플리카셋 직접 생성하기
  ```yaml
  apiVersion: apps/v1
  kind: ReplicaSet # 디플로이먼트와 정의 내용이 거의 같다.
  metadata:
    name: whoami-web
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: whoami-web
    template:
  ```
  - 디플로이먼트 정의와 차이점은 replicas 필드가 있다.
    - 1 -> 레플리카셋은 파드를 한 개만 실행

- 실습
  - 레플리카셋 배치하기
  - LB 서비스 함께 배치하되 파드에 트래픽 전달되도록 두 리소스 레이블 셀렉터 동일하게 지정
  - cd ch06
  - kubectl apply -f whoami/
  - kubectl get replicaset whoami-web
  - curl $(kubectl get svc whoami-web -o jsonpath='http://{.status.loadBalancer.ingress[0].*}:8088') -UseBasicParsing
    - 서비스로 http get 요청 전달
  - kubectl delete pods -l app=whoami-web
    - 파드 모두 삭제
  - curl $(kubectl get svc whoami-web -o jsonpath='http://{.status.loadBalancer.ingress[0].*}:8088') -UseBasicParsing
  - kubectl describe rs whoami-web
  
- 실습
  - 레플리카 수 세 개로 지정, 반영 후 앱 스케일링
  - kubectl apply -f whoami/update/whoami-replicas-3.yaml
  - kubectl get pods -l app=whoami-web
  - kubectl delete pods -l app=whoami-web
  - kubectl get pods -l app=whoami-web
  - curl $(kubectl get svc whoami-web -o jsonpath='http://{.status.loadBalancer.ingress[0].*}:8088') -UseBasicParsing
  
로드밸런스에 의해 요청마다 다른 파드에서 응답을 하고 있다.  
어떻게 앱에 스케일링이 빨리 적용되고, HTTP 응답이 LB 되어 다른 파드들에서 올까?  
실습 환경은 단일 클러스터라 모든 파드가 하나의 노드에서 실행되어 이미지를 받는 시간이 필요하다.  
즉 노드에서 이미지를 내려 받기 전, 스케일링이 이미 적용된다.  
http 요청이 여러 파드에서 실행 가능한 이유는 **서비스와 파드간 느슨한 결합**에 있다.  
서비스, 레플리카셋이 레이블 셀렉터를 통해 동일 레이블 리소스를 찾는 덕분에 관련된 모든 파드가 엔드포인트로 추가된다.  
레플리카셋 -> 레이블 셀렉터로 자신이 관리하는 파드 식별  
  
K8s의 스케일링이 간단한 이유는 네트워킹(서비스)과 컴퓨팅(파드)을 분리한 추상화 덕분이다.  
K8s의 모든 유형의 서비스는 LB 기능을 갖고 있다.  
서비스는 클러스터로 들어오는 트래픽을 받아 연결된 파드로 전달하는 역할을 한다.  
클러스터 내부 통신은 클러스터IP 서비스가 담당하고, LB 기능을 제공한다.  
- 실습
  - 파드 수동 배치
  - 파드에 whoami-web 서비스의 클러스터 IP 통해 해당 서비스 호출하기
  - kubectl apply -f sleep.yaml
  - kubectl get svc whoami-web
  - kubectl exec deploy/sleep -- sh -c 'nslookup whoami-web | grep "^[^*]"'
  - kubectl exec deploy/sleep -- sh -c 'for i in 1 2 3; do curl -w \\n -s http://whoami-web:8088; done;'
  
클러스터 IP를 통해 접근해도 LB는 적용된다.  
whoami-web.default.svc.cluster.local에서 "cluster.local" 부분은 클러스터 내부 도메인이다.  
외부 도메인과는 별개로 클러스터 내부에서 파드와 서비스가 서로 쉽게 통신 가능하도록 사용된다.  
클러스터 내부에서만 유효하며, K8s 내에 여러 클러스터가 존재하는 경우 네임스페이스 충돌 방지를 위해 변경해서 사용 될 수 있다.  

# 6.2 디플로이먼트와 레플리카셋을 이용한 부하 스케일링
레플리카셋 이용시 앱 스케일링 난이도가 매우 쉽다.  
단지 yaml 정의에 수만 조절하면 된다.  
작은 규모의 무상태 컴포넌트일수록 최적 환경이다.  
기능 별로 컴포넌트로 분할 후 개별 업데이트 및 스케일링 적용 분산 아키텍처를 채용하는 이유이기도 하다.  
  
디플로이먼트는 유용한 관리 계층이 추가된다.  
앱 정의 수단으로는 레플리카셋보다 디플로이먼트를 우선 선택한다.  
디플로이먼트에 스케일링 적용시 replicas 필드를 명시해야 한다.  
- 예제 6-2 여러 개 레플리카 배치 디플로이먼트 정의
  ```yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: pi-web
  spec:
    replicas: 2 # 필수 명시는 아니지만, 생략할 경우 default 1
  selector:
    matchLables:
      app: pi-web
    template:
  ```
  - 디플로이먼트 파드 정의 변경 -> 대체 레플리카셋 생성 후 기존 레플리카셋 레플리카 수 0으로 만든다.
- 실습
  - 원주율 웹 앱 배치 디플로이먼트와 서비스 생성
  - 앱 업데이트 후 레플리카셋 반응 살펴보기
  - kubectl apply -f pi/web/
    - 앱 배치
  - kubectl get rs -l app=pi-web
    - 레플리카셋 상태 확인
  - kubectl apply -f pi/web/update/web-replicas-3.yaml
    - 레플리카 수 증가, 스케일링 적용
  - kubectl get rs -l app=pi-web
  - kubectl apply -f pi/web/update/web-logging-level.yaml
  - kubectl get rs -l app=pi-web
  
파드 정의 변경 -> 기존 포함 파드 -> 디플로이먼트는 새 레플리카셋 생성 후 새 파드 준비 후 기존 파드 삭제  
kubectl scale 명령을 통해 컨트롤러 리소스 스케일링 바로 시작 가능하지만, yaml 정의대로 되도록 납두자.  
성능 문제가 있고 자동 재배치가 오래 걸린다면 kubectl scale 명령 대응이 나을 수도 있다.  
scale 후 yaml 파일도 맞춰 수정 필요  
- 실습
  - 원주율 웹 앱 kubect scale 명령 사용하여 스케일링하기
  - kubectl scale --replicas=4 deploy/pi-web
    - 원주율 앱 신속하게 스케일링 해야 한다.
  - kubectl get rs -l app=pi-web
  - kubectl apply -f pi/web/update/web-replicas-3.yaml
    - 로그 수준 원래대로 돌리기
  - kubectl get rs -l app=pi-web
  - kubectl get pods -l app=pi-web
  
수동 적용 스케일링 효과는 점진적으로 풀린다.  
새 레플리카셋을 만드는 대신 기존 레플리카셋 재활용 -> 효율을 위한 선택, 레이블 사용시 가능하다.  
디플로이먼트가 랜덤하게 생성한 파드 이름은 디플로이먼트 정의에 포함된 템플릿 속 파드 정의 해시값이다.  

디플로이먼트와 파드가 관계를 유지하는 내부 과정 이해하면 레플리카 수 0인 레플리카셋이 여러 개 남아있는 이유를 알 수 있다.  
웹, 프록시 컨테이너 모두 스케일링과 함께 LB 효과를 얻는 방법도 있다.  
- 실습
  - 레플리카 수 두 개인 프록시 디플로이먼트 생성
  - 프록시를 원주율 앱과 통합 해줄 서비스, 컨피그맵 함께 배치
  - kubectl apply -f pi/proxy/
  - kubectl get svc whoami-web -o jsonpath='http://{.status.loadBalancer.ingress[0].*}:8080/?dp=10000'
  - 브라우저 -> 앱 접근, 소숫점 바꾸어 가며 원주율 구하기
  - kubectl logs -l app=pi-proxy --tail 1

# 6.3 데몬셋을 이용한 스케일링으로 고가용성 확보하기
데몬셋은 리눅스 백그라운드에서 단일 인스턴스로 동작한다. -> 시스템 관련 정보  
K8s의 데몬셋은 클러스터 내 일부 노드에서 단일 레플리카 또는 파드로 동작하는 리소스다.  
각 노드마다 파드가 하나씩 동작하며 해당 노드 데이터를 수집한다.  
- 예제 6-3 
  - 프록시 용도로 사용할 데몬셋
  ```yaml
  apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    name: pi-proxy
  spec:
    selector:
      matchLabels: 
        pp: pi-proxy # 데몬셋의 관리 대상인 파드를 결정하는 기준
  template:
    metadata:
      labels:
        app: pi-proxy # 레이블 셀렉터와 레이블이 일치해야 한다.
    spec:
  ```
  - 위 정의에도 호스트경로 볼륨이 쓰인다.
    - 각 파드가 자신만의 캐시를 갖는다. -> 최대 성능 X[2022-10-01-smt-0-2a](..%2F..%2F..%2F..%2F..%2F..%2FDownloads%2F2022-10-01-smt-0-2a)
- 실습
  - 기존 컨트롤러 리소스 유형 변경 할 순 없다.
  - 앱을 망가뜨리지 않으며 데몬셋 교체
  - kubectl apply -f pi/proxy/daemonset/nginx-ds.yaml
    - 데몬셋 배치
  - kubectl get endpoints pi-proxy
    - 프록시 서비스에 등록된 엔드포인트 확인
  - kubectl delete deploy pi-proxy
  - kubectl get daemonset pi-proxy
  - kubectl get po -l app=pi-proxy
  
deploy 리소스 삭제 전 데몬셋을 배치하면 deploy 삭제시 파드가 사라지지 않는다.  
데몬셋이 생성한 파드도 서비스의 엔드포인트가 추가 된다.  
실습 환경은 단일 노드인데, 노드가 늘어나면 그 수 만큼 파드가 늘어난다.  
- 실습
  - 수동으로 프록시 파드 삭제 -> 데몬셋이 대체 파드를 생성한다
  - kubectl get ds pi-proxy
  - kubectl delete po -l app=pi-proxy
  - kubectl get po -l app=pi-proxy
  
- 예제 6-4
  - 일부 노드를 대상으로 하는 데몬셋
  ```yaml
  spec:
    containers:
      # ...
    volumes:
      # ...
    nodeSelector: # 특정 노드에서만 파드 실행
      kiamol: ch06 # kiamol=ch06 레이블 부여 노드만 대상 -> 파드 실행
  ```
- 실습
  - nodeSelector 추가 정의 데몬셋 업데이트
  - 레이블 일치 노드 없음 -> 기존 파드 제거
  - 대상 노드로 만들어 새파드 생성
  - kubectl apply -f pi/proxy/daemonset/nginx-ds-nodeSelector.yaml
  - kubectl get ds pi-proxy
  - kubectl get po -l app=pi-proxy
  - kubectl label node $(kubectl get nodes -o jsonpath='{.items[0].metadata.name}') kiamol=ch06 --overwrite
    - 셀렉터와 일치하는 레이블을 노드에 부여
  - kubectl get ds pi-proxy
- 실습
  - kubectl delete 명령에 cascade 옵션이 있다. -> 컨트롤러 리소스만 삭제
  - 남겨진 관리 대상 리소스는 셀렉터 일치 새 컨트롤러가 생성되면 관리 대상으로 들어간다.
  - kubectl delete ds pi-proxy --cascade=false
    - 관리 대상 파드는 남기고, 데몬셋 삭제
  - kubectl get po -l app=pi-proxy
  - kubectl apply -f pi/proxy/daemonset/nginx-ds-nodeSelector.yaml
  - kubectl get ds pi-proxy
  - kubectl get po -l app=pi-proxy
  - kubectl delete ds pi-proxy
  - kubectl get po -l app=pi-proxy
  
관리 대상 리소스를 남긴채 컨트롤러 리소스만 삭제하는 것은 자주 사용하지 않아도, 필요 할 땐 유용하게 쓸 수 있다.  
기존 파드는 문제 없는데, 노드 유지보수 작업 실행시 -> 불필요하게 파드 삭제 생성 보다 데몬셋을 이용해 간단하게 다시 생성이 수월하다.  
데몬셋은 고가용성을 위한 리소스지만, 적절한 상황이 제한적이다.

# 6.4 쿠버네티스의 객체 간 오너십
K8s는 관리 주체가 사라진 객체를 찾아 제거하는 GC가 있다.  
- 실습
  - 모든 파드와 레플리카셋의 메타데이터에서 관리 주체 리소스 정보 확인
  - kubectl get po -o custom-columns=NAME:'{.metadata.name}',OWNER:'{.metadata.ownerReferences[0].name}',OWNER_KIND:'{.metadata.ownerReferences[0].kind}'
    - 각 파드 관리 주체 리소스 확인
  - kubectl get rs -o custom-columns=NAME:'{.metadata.name}',OWNER:'{.metadata.ownerReferences[0].name}',OWNER_KIND:'{.metadata.ownerReferences[0].kind}'
    - 각 레플리카셋의 관리 주체 리소스 확인
  
이들 관계를 형성하는 수단은 레이블 셀렉터뿐이다.  
- 실습
  - 최상위 객체 -> kiamol 삭제 -> 관리 대상 리소스 모두 삭제
  - kubectl delete all -l kiamol=ch06

# 6.5 연습 문제

# 해당 파트 사용 명령어 모음
- kubectl apply -f whoami/
- kubectl get replicaset whoami-web
- curl $(kubectl get svc whoami-web -o jsonpath='http://{.status.loadBalancer.ingress[0].*}:8088') -UseBasicParsing
- kubectl delete pods -l app=whoami-web
- curl $(kubectl get svc whoami-web -o jsonpath='http://{.status.loadBalancer.ingress[0].*}:8088') -UseBasicParsing
- kubectl describe rs whoami-web
- kubectl apply -f whoami/update/whoami-replicas-3.yaml
- kubectl get pods -l app=whoami-web
- kubectl delete pods -l app=whoami-web
- kubectl get pods -l app=whoami-web
- curl $(kubectl get svc whoami-web -o jsonpath='http://{.status.loadBalancer.ingress[0].*}:8088') -UseBasicParsing
- kubectl apply -f sleep.yaml
- kubectl get svc whoami-web
- kubectl exec deploy/sleep -- sh -c 'nslookup whoami-web | grep "^[^*]"'
- kubectl exec deploy/sleep -- sh -c 'for i in 1 2 3; do curl -w \\n -s http://whoami-web:8088; done;'
- kubectl apply -f pi/web/
- kubectl get rs -l app=pi-web
- kubectl apply -f pi/web/update/web-replicas-3.yaml
- kubectl get rs -l app=pi-web
- kubectl apply -f pi/web/update/web-logging-level.yaml
- kubectl get rs -l app=pi-web
- kubectl scale --replicas=4 deploy/pi-web
- kubectl get rs -l app=pi-web
- kubectl apply -f pi/web/update/web-replicas-3.yaml
- kubectl get rs -l app=pi-web
- kubectl get pods -l app=pi-web
- kubectl apply -f pi/proxy/
- kubectl get svc whoami-web -o jsonpath='http://{.status.loadBalancer.ingress[0].*}:8080/?dp=10000'
- kubectl logs -l app=pi-proxy --tail 1
- kubectl apply -f pi/proxy/daemonset/nginx-ds.yaml
- kubectl get endpoints pi-proxy
- kubectl delete deploy pi-proxy
- kubectl get daemonset pi-proxy
- kubectl get po -l app=pi-proxy
- kubectl get ds pi-proxy
- kubectl delete po -l app=pi-proxy
- kubectl get po -l app=pi-proxy
- kubectl apply -f pi/proxy/daemonset/nginx-ds-nodeSelector.yaml
- kubectl get ds pi-proxy
- kubectl get po -l app=pi-proxy
- kubectl label node $(kubectl get nodes -o jsonpath='{.items[0].metadata.name}') kiamol=ch06 --overwrite
- kubectl get ds pi-proxy
- kubectl delete ds pi-proxy --cascade=false
- kubectl get po -l app=pi-proxy
- kubectl apply -f pi/proxy/daemonset/nginx-ds-nodeSelector.yaml
- kubectl get ds pi-proxy
- kubectl get po -l app=pi-proxy
- kubectl delete ds pi-proxy
- kubectl get po -l app=pi-proxy
- kubectl get po -o custom-columns=NAME:'{.metadata.name}',OWNER:'{.metadata.ownerReferences[0].name}',OWNER_KIND:'{.metadata.ownerReferences[0].kind}'
- kubectl get rs -o custom-columns=NAME:'{.metadata.name}',OWNER:'{.metadata.ownerReferences[0].name}',OWNER_KIND:'{.metadata.ownerReferences[0].kind}'
- kubectl delete all -l kiamol=ch06