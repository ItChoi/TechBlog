# 목차
- 1파트 쿠버네티스 빠르게 훑어보기 (5장)
    - 5.1 쿠버네티스에서 컨테이너 파일 시스템이 구축되는 과정
    - 5.2 볼륨과 마운트로 노드에 데이터 저장하기
    - 5.3 전체에서 접근 가능하도록 데이터 저장하기: 영구볼륨과 클레임
    - 5.4 스토리지의 유형과 동적 볼륨 프로비저닝
    - 5.5 스토리지를 선택할 때 고려할 점
    - 5.6 연습 문제

# 서론
고장을 일으킨 노드에서 실행되던 파드에 데이터가 저장되는 상황이라면,  
파드는 yaml 정의에 의해 새로 생성되지만, 기존 파드 데이터에 접근 할 수 없다. 중요 데이터라면 막대한 손실을 유발한다.  
따라서 어떤 노등 있는 파드든 동일하게 데이터 접근 가능한 저장소는 반드시 필요하다.  
  
K8s에 전체 클러스터에서 사용 가능한 스토리지 내장 기능은 없다.  

# 5.1 쿠버네티스에서 컨테이너 파일 시스템이 구축되는 과정
컨테이너의 파일 시스템은 여러 출처를 합쳐 구성된다.
- 이미지 파일 시스템
  - 애플리케이션 레이어
  - 운영체제 레이어
- 컨테이너에 기록 가능한 레이어 (도커 이미지는 읽기 전용, 이미지 파일 수정 가능하게 한 메커니즘)
  - 이미지 파일 수정
  - 새로운 파일 기록
  
파드 속 컨테이너는 자신만의 파일 시스템을 갖고, k8s에 의해 구성된다.  
앱의 데이터 기록이 필요하다면 그 플로우를 이해하고 파드에 정의해야 한다.  
그래야 파드 재시작 또는 새로 생성될 때 데이터 손실 발생을 줄일 수 있다.  
- 실습
  - 컨테이너 앱 충돌을 일으켜 컨테이너 종료
  - 대체할 새로운 파드가 생성된다.
  - 새 컨테이너는 기존 컨테이너의 기록 가능 레이어에 기록한 데이터 유실 발생
  - cd ch05
  - kubectl apply -f sleep/sleep.yaml
  - kubectl exec deploy/sleep -- sh -c 'echo ch05 > /file.txt; ls /*.txt'
    - 컨테이너 속 파일 하나 생성
  - kubectl get pod -l app=sleep -o jsonpath='{.items[0].status.containerStatuses[0].containerID}'
    - 컨테이너 ID 확인
  - kubectl exec -it deploy/sleep -- killall5
    - 파드 재시작 컨테이너 모든 프로세스 강제 종료
  - kubectl get pod -l app=sleep -o jsonpath='{.items[0].status.containerStatuses[0].containerID}'
    - 대체된 컨테이너 ID 확인
  - kubectl exec deploy/sleep -- ls /*.txt
  
파드 재시작은 컨테이너의 재시작이다 -> 파드에서 기록된 것이 재시작 후 데이터 유실 될 수 있다.  
  
컨피그맵과 비밀 값은 읽기 전용 스토리지 단위였다.  
K8s는 이외에도 기록 가능한 유형의 볼륨이 여러 가지 있다.  
볼륨에 데이터 저장시 파드 재시작 또는 신규 생성으로 인한 데이터 손실을 막을 수 있다.  
볼륨도 파드 수준에서 정의되는 컨테이너 파일 시스템 구성 수단 중 하나다.  
- 예제
  - emptyDir: {} 빈 디렉터리로 초기화되는 유형의 볼륨인 공디렉터리
  ```yaml
  spec:
    containers:
      - name: sleep
        image: kiamol/ch03-sleep
        volumeMounts:
          - name: data # 이름이 data인 볼륨을 마운트
            mountPath: /data # 이 볼륨을 경로 /data에 마운트
    volumes:
      - name: data # 볼륨 data의 정의
        emptyDir: {} # 이 볼륨의 유형은 공 디렉터리
  ```
  - 공 디렉터리는 파드와 같은 생애 주기를 갖기 때문에 유용하게 쓸 수 있다.
    - 공 디렉터리 볼륨에 저장된 데이터는 파드 재시작, 새 컨테이너 -> 데이터 유지
- 실습
  - sleep 디플로이먼트를 공 디렉터리 볼륨이 추가된 정의로 업데이트
  - 컨테이너 종료 및 재시작, 생성 시 이 전 컨테이너 데이터 손실을 막을 수 있다.
  - kubectl apply -f sleep/sleep-with-emptyDir.yaml
  - kubectl exec deploy/sleep -- ls /data
  - kubectl exec deploy/sleep -- sh -c 'echo ch05 > /data/file.txt ls /data'
  - kubectl get pod -l app=sleep -o jsonpath='{.items[0].status.containerStatuses[0].containerID}'
  - kubectl exec deploy/sleep -- killall5
  - kubectl get pod -l app=sleep -o jsonpath='{.items[0].status.containerStatuses[0].containerID}'
  - kubectl exec deploy/sleep -- cat /data/file.txt
  
공 디렉터리 볼륨은 임시 저장이 목적이라면 모든 앱에서 사용 가능하다.  
즉 공 디렉터리 볼륨은 로컬 캐시에 적합하다.  
공 디렉터리 볼륨은 파드와 생애주기가 같다. -> 재시작시 다시 공 디렉터리가 된다.  
파드 재시작 후 데이터 유지를 원한다면, 파드와 별도의 생애 주기를 가진 유형의 볼륨을 마운트해야 한다.  

# 5.2 볼륨과 마운트로 노드에 데이터 저장하기
데이터를 특정 노드에 고정 할 지 여부를 결정해야 한다.  
고정한다면 대체파드 또한 이전 파드와 동일한 노드에만 배치해야 한다.  
고정하지 않는다면 당연하게도, 파드는 노드에 랜덤하게 배치된다.  
  
K8s를 통해 다양한 데이터 배치 선택지가 있다.  
선택지 중 가장 간단한 것은 노드의 특정 디렉터리를 가리키는 볼륨이다.  
컨테이너가 볼륨 마운트 경로에 데이터 기록한다는 것은 노드의 특정 디렉터리가 된다.  
- 실습
  - 성능 향상을 위해 프록시를 갖춘 웹 앱 실행
  - 이 앱은 파드 하나로 실행, 외부로 노출되지 않은 서비스와 연결
  - 프록시는 별도 파드에서 실행, LB 서비스를 사용해 외부로 노출
  - kubectl apply -f pi/v1/
    - 파이 앱 배치
  - kubectl wait --for=condition=Ready pod -l app=pi-web
    - 파이 파드 준비 상태까지 대기
  - kubectl get svc pi-proxy -o jsonpath='http://{.status.loadBalancer.ingress[0].*}:8080/?dp=30000'
  - web - url 접근 후 새로고침
  - kubectl exec deploy/pi-proxy -- ls -l /data/nginx/cache
  
프록시가 로컬 캐시에 저장된 응답을 직접 제공, 웹 서버 접근 X  
웹 앱에서 흔히 볼 수 있는 설정이다.  
실습에서 공 디렉터리 볼륨이 마운트된 디렉터리로, 캐시된 데이터를 저장한다.  
이런 예시는 공디렉터리 볼륨이 적합하다. -> 볼륨 저장 데이터 중요도가 높지 않다. -> 파드 대체는 캐시 유실  
- 실습
  - 프록시 파드 제거하기
  - 새 파드 생성 -> 공 디렉터리는 초기 상태로 돌아간다.
  - 웹 파드에 요청 (로컬 캐시 X)
  - kubectl delete pod -l app=pi-proxy
    - 프록시 파드 제거
  - kubectl exec deploy/pi-proxy -- ls -l /data/nginx/cache
    - 새로 생성된 파드 캐시 디렉터리 내용 확인
  - 파이 웹 접속 -> 새로고침
  
파드가 대체되면, 공 데릭터리 볼륨도 새로 생성된다.  
공 디렉터리보다 더 오래 유지되는 볼륨은 노드의 디스크를 가리키는 것이다.  
노드 디스크 볼륨 -> 호스트경로 볼륨이라 한다.  
호스트 경로 볼륨도 파드에 정의되고, 컨테이너 파일 시스템에 마운트되는 형태이다.  
컨테이너 마운트 경로 디렉터리 데이터 기록 -> 노드 디스크에 실제 데이터 기록  
즉 파드에 호스트 경로 볼륨 정의 -> 컨테이너 파일 시스템에 마운트 -> 실제 데이터는 노드 디스크에 기록  
  
호스트 경로 볼륨도 잘 사용하면 유용하지만, 한계점을 잘 이해하고 사용해야 한다.  
K8s가 클러스터의 모든 노드에 동일 데이터 복사본을 만들어주진 않는다.  
- 예제 5-2
  - 호스트 경로 볼륨 사용하도록 수정된 파드 정의
  - 프록시 컨테이너가 /data/nginx/cache 디렉터리에 캐시 파일 기록
  - 실제 데이터 기록은 노드 파일 시스템 중 /volumes/nginx/cache 디렉터리
  ```yaml
  spec: 
    containers:
      - image: nginx:1.17-alpine
        name: nginx
        ports:
          - containerPort: 80
        volumeMounts:
          - name: cache-volume
            mountPath: /data/nginx/cache # 프록시 캐시 저장 경로
    volumes:
      - name: cache-volume
        hostPath: # 노드 디렉터리 사용
          path: /volumes/nginx/cache # 사용할 노드 디렉터리
          type: DirectoryOrCreate # 디렉터리 없으면 생성
  ```
  - 위 방식 사용시 파드가 항상 같은 노드 사용한다면 볼륨 생애주기가 노드 디스크와 같다.
    - 노드가 하나 뿐인 환경에서 보장되지만, 여러 노드가 있는 경우 보장 X
  - 새로 생성된 대체 파드는 시작 시 호스트 경로 볼륨을 읽는다. 
-  실습
  - 프록시 디플로이먼트를 예제 5-2 정의로 업데이트
  - 업데이트된 파드는 기존 캐시 사용하여 응답 가능하다.
  - kubectl apply -f pi/nginx-with-hostPath.yaml
    - 호스트 경로 볼륨 사용하는 프록시 파드로 업데이트
  - kubectl exec deploy/pi-proxy -- ls -l /data/nginx/cache
    - 프록시 파드 속 캐시 디렉터리 내용 확인
  - 웹 접근 
  - kubectl delete pod -l app=pi-proxy
    - 프록시 파드 강제 삭제
  - kubectl exec deploy/pi-proxy -- ls -l /data/nginx/cache
    - 새로 생성된 프록시 파드 캐시 디렉터리 내용 확인
  
호스트경로 볼륨은 클러스터 내에 노드가 두 개 이상 있을 때 부터 문제가 생길 수 있다.  
파드 정의에서 항상 같은 노드에 존재하게 할 수 있다. 그러나 자기수복성과 트레이드오프를 해야 한다.  
즉 노드 고장으로 인해 파드가 실행되지 않아 앱이 제한 될 수 있다.  
그리고 보안 취약점이 드러난다. -> 파드 컨테이너가 노드의 파일 시스템 전체에 접근 가능하게 된다.  
- 예제 5-3
  - 노드의 파일 시스템 전체에 접근 가능한 파드
  ```yaml
  spec:
    containers:
      - name: sleep
        image: kiamol/ch03-sleep
        volumeMounts:
          - name: node-root
            mountPath: /node-root
    volumes:
      - name: node-root
        hostPath:
          path: / # 노드 파일 시스템의 루트 디렉터리
          type: Directory # 경로에 디렉터리가 존재해야 한다
  ```
  - 위 파드 생성 권한이 있는 누구든지 파드가 동작 중인 노드의 파일 시스템 전체에 접근 가능하다.
  - 노드 전체가 장악당하지 않으려면 조심해야 한다.
- 실습 
  - 예제 5-3 정의된 파드 실행
  - 파드 컨테이너 명령을 통해 노드 파일 시스템 조회해보기
  - kubectl apply -f sleep/sleep-with-hostPath.yaml
    - 호스트 경로 볼륨이 마운트된 파드 실행
  - kubectl exec deploy/sleep -- ls -l /var/log
    - 컨테이너 속 로그 파일 확인
  - kubectl exec deploy/sleep -- ls -l /node-root/var/log
    - 노드 파일 시스템의 로그 파일 내용 확인
  - kubectl exec deploy/sleep -- whoami
    - 컨테이너 사용자명 확인
  
파드 컨테이너에서 노드에 있는 K8s 로그를 읽었다.  
예전에 개발된 앱 중에 실행 중 노드의 특정 경로에 접근해야 하는 경우도 있었다.  
다만 볼륨 마운트 시 하위 디렉터리를 마운트 하여 필요 이상으로 노드의 파일 시스템을 노출시킬 필요는 없다.  
- 예제 5-4
  - 노드의 파일 시스템을 최소한 노출하는 호스트 경로 볼륨 정의
  ```yaml
  spec:
    containers:
      - name: sleep
        image: kiamol/ch03-sleep
        volumeMounts:
          - name: node-root # 마운트할 볼륨 이름
            mountPath: /pod-logs # 마운트 대상 컨테이너 경로
            subPath: var/log/pods # 마운트 대상 볼륨 내 경로
          - name: node-root
            mountPath: /container-logs
            subPath: var/log/containers
    volumes:
      - name: node-root
        hostPath:
          path: /
          type: Directory
  ```
  - 볼륨 정의는 여전히 호스트의 루트
  - 컨테이너에서 볼륨에 접근 가능 유일 통로는 하위 디렉터리를 대상으로 한다.
- 실습
  - 예제 5-4 실행, 컨테이너가 제한된 범위의 노드 파일 시스템 조회하도록 정의
  - kubectl apply -f sleep/sleep-with-hostPath-subPath.yaml
    - 파드 업데이트
  - kubectl exec deploy/sleep -- sh -c 'ls /pod-logs | grep _pi-'
    - 노드 파일 시스템에서 파드 로그 확인
  - kubectl exec deploy/sleep -- sh -c 'ls /container-logs | grep nginx'
  
호스트 경로 볼륨은 stateful 앱을 K8s로 첫 도입 시 유리하다.  

# 5.3 전체에서 접근 가능하도록 데이터 저장하기: 영구볼륨과 클레임
K8s 볼륨 유형 중 분산 스토리지 시스템 지원을 받는 것이 여럿 있다.  
AKS 클러스터 -> 애저 파일스, 애저 디스크 사용 가능  
EKS 클러스터 -> 일래스틱 블록 스토어 사용 가능
온프레미스 환경 -> NFS(Network File System), 글러스터FS(GlusterFS) 등의 네트워크 파일 시스템 사용 가능  
  
필수 설정은 각기 다르고, 파드 정의에 기술도 가능하다.  
하지만 특정 스토리지 솔루션 의존성이 높아진다.  
K8s는 스토리지 솔루션과 느슨한 결합을 유지하는 유연성 있는 수단을 제공한다.  
  
앞서 파드는 컴퓨팅 계층의 추상, 서비스는 네트워크 계층의 추상이라 했다.  
스토리지 계층의 추상은 '영구 볼륨'과 '영구볼륨클레임'이 있다.  
- 예제 5-5 NFS 스토리지 사용 볼륨 정의
  ```yaml
  apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: pv01 # 볼륨 이름
  spec:
    capacity:
      storage: 50Mi # 볼륨 용량
    accessModes: # 파드 접근 유형
      - ReadWriteOnce # 파드 하나에서만 사용가능
    nfs: # NFS 스토리지를 사용하는 볼륨
      server: nfs.my.network # NFS 서버 도메인 이름
      path: "/kubernetes-volumes" # 스토리지 경로
  ```
- 실습 
  - 로컬 스토리지 사용 영구 볼륨 생성
  - 영구 볼륨은 전체 클러스터에서 접근 가능, 볼륨은 한 노드에만 있다.
  - 영구 볼륨은 자신이 실제 위치한 노드와 잘 연결되어 있어야 한다.
  - 레이블을 활용하여 노드와 볼륨 연결
  - kubectl label node $(kubectl get nodes -o jsonpath='{.items[0].metadata.name}') kiamol=ch05
    - 클러스터의 첫 번째 노드에 레이블 부여
  - kubectl get nodes -l kiamol=ch05
    - 레이블 셀렉터로 노드 존재 확인
  - kubectl apply -f todo-list/persistentVolume.yaml
    - 레이블 부여 노드의 로컬 볼륨 사용하는 영구볼륨 배치
  - kubectl get pv
    - 영구 볼륨 상세 정보 확인
  
영구 볼륨 생성 완료, 접근 유형과 용량도 지정됐지만, 파드가 영구 볼륨을 직접 사용하진 못한다.  
영구볼륨클레임(PVC)을 사용하여 볼륨 사용을 요청해야 한다.  
PVC는 파드가 사용하는 스토리지의 추상이다. -> 앱에서 사용할 스토리지 요청 역할  
PVC는 요구 조건과 일치하는 영구 볼륨과 함꼐 쓰인다. 상세 볼륨 정보는 PV에 위임한다.  

PVC가 일치하는 PV가 있다면 이 PV와 연결된다.  
PV와 PVC는 1:1 관계이며, PVC와 연결된 PV는 다른 PVC와 연결 될 수 없다.  
- 실습
  - 예제 5-6 정의된 PVC 배치
  - 우리가 만든 PV는 PVC 요구조건을 만족한다.
  - PVC와 PV는 연결된다.
  - kubectl apply -f todo-list/postgres-persistentVolumeClaim.yaml
    - PV와 연결된 PVC 생성
  - kubectl get pvc
    - PVC 목록 확인
  - kubectl get pv
    - 영구 볼륨 목록 확인
  
위 실습은 PV를 명시적으로 생성해야 하는 정적 프로비저닝 방식이다.  
요구 사항 일치 PV가 없을 때 PVC를 배치하는 경우 스토리지를 사용할 수 없다.  
요구 사항 만족 PV가 생길 때 까지 대기 상태로 남는다.  
- 실습
  - PVC 하나 더 배치 -> 대기 상태로 남는다. (PV 없음)
  - kubectl apply -f todo-list/postgres-persistentVolumeClaim-too-big.yaml
    - PV가 없는 상태로 PVC 배치
  - kubectl get pvc
  
요구 조건이 100MB 이상인 PV가 생성 될 때 까지 대기 상태로 남는다.  
파드가 PVC를 사용하려면 PV와 연결된 상태여야 한다.  
- 예제 5-7
  - PVC 사용 파드 정의
  ```yaml
  spec:
    containers:
      - name: db
        image: postgres:11.6-alpine
        volumeMounts:
          - name: data
            mountPath: /var/lib/postgresql/data
    volumes:
      - name: data
        persistentVolumeClaim: # PVC 볼륨으로 사용
          claimName: postgres-pvc # 사용 PVC 이름
  ```
  - 볼륨 포함 PostgreSQL DB 파드 배치 준비 끝
  - 파드와 PVC 정의만 담당하면 PVC를 신경 쓰지 않아도 된다.
  - 하지만 실습 환경에선 신경써보자. 
    - 한 단계 추가 -> 노드에서 볼륨이 사용할 디렉터리 경로 생성
- 실습
  - 실제 클러스터는 노드에 로그인 권한이 없는 경우가 많다.
  - 우회를 위해 호스트 경로 마운트로 노드 루트 디렉터리를 마운트한 파드 이용하여 노드 파일 시스템에 디렉터리 생성
  - kubectl apply -f sleep/sleep-with-hostPath.yaml
    - 노드의 디스크 접근 가능 파드 실행
  - kubectl wait --for=condition=Ready pod -l app=sleep
  - kubectl exec deploy/sleep -- mkdir -p /node-root/volumes/pv01
  
노드에 직접적인 권한이 없어도 위 명령어를 통해 디렉터리 생성이 가능하다.  
- 실습
  - DB 파드 배치
  - DB 파일 초기화 -> 볼륨에 어떤 파일 생성 됐는지 체크
  - kubectl apply -f todo-list/postgres/
    - DB 파드 배치
  - sleep 30
    - DB 파일 초기화까지 대기
  - kubectl logs -l app=todo-db --tail 1
    - DB 파드 로그 확인
  - kubectl exec deploy/sleep -- sh -c 'ls -l /node-root/volumes/pv01 | grep wal'
    - 볼륨에 생성된 파일 체크
- 실습
  - PostgreSQL DB 실제 사용 to-do 앱 파드 실행
  - kubectl apply -f todo-list/web/
    - 앱 웹 파드 배치
  - kubectl wait --for=condition=Ready pod -l app=todo-web
  - kubectl get svc todo-web -o jsonpath='http://{.status.loadBalancer.ingress[0].*}:8081/new'
    - 앱 URL 확인
  - 웹 브라우저 접근 - 새 할 일 추가
  - kubectl delete pod -l app=todo-db
    - DB 파드 강제 삭제
  - kubectl exec deploy/sleep -- ls -l /node-root/volumes/pv01/pg_wal
    - 볼륨 기록 데이터 확인
  
RDBMS를 꼭 K8s 클러스터 안에서 기동해야하는 이유가 뭘까?  
먼저 스토리지 유형에 따라 클러스터가 동적으로 볼륨 관리하는 방법부터 알아보자.

# 5.4 스토리지의 유형과 동적 볼륨 프로비저닝
명시적 PV, PVC를 생성해 연결하는 정적 볼륨 프로비저닝 방식은 모든 K8s 클러스터에서 사용 할 수 있다.  
하지만 대부분의 K8s 플랫폼에서 동적 볼륨 프로비저닝이라는 더 간단한 방식을 제공한다.  
  
동적 볼륨 프로비저닝은 PVC만 생성하면 PV를 클러스터에서 동적으로 생성해준다.  
클러스터는 다양한 요구사항에 맞춰 여러 스토리지 유형을 설정 할 수 있다.  
PVC 정의에 스토리지 유형만 지정하면 된다.  
- 예제 5-8
  - 동적 PVC
  ```yaml
  apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    name: postgres-pvc-dynamic
  spec:
    accessModes:
      - ReadWriteOnce
    resources:
      requests:
        storage: 10Mi
        # storageClassName 필드가 없으면 기본 유형이 쓰인다
  ```
  - PV를 따로 만들지 않아도 K8s가 PV를 동적으로 생성해서 PVC와 연결한다. (클러스터에 따라 다르다.)
    - K8s 플랫폼이 동적 볼륨 프로비저닝 지원하고, 기본 스토리지 유형이 지정되어 있다면 해당한다. 
- 실습
  - 동적 볼륨 프로비저닝 적용 PVC 배치
  - kubectl apply -f todo-list/postgres-persistentVolumeClaim-dynamic.yaml
    - PVC 배치
  - kubectl get pvc;
  - kubectl get pv;
  - kubectl delete pvc postgres-pvc-dynamic
    - pvc 삭제시 pv도 함께 삭제된다.
  - kubectl get pv;
  
도커 데스크톱 K8s 플랫폼의 경우 동적 볼륨 프로비저닝 기본 스토리지 유형은 '호스트경로 볼륨'이다.  
AKS는 '파일스', K3s는 '호스트 경로 볼륨' 등을 사용하며 동일한 유형을 쓰더라도 플랫폼마다 동작이 다를 수 있다.  
  
스토리지 유형은 상당히 유연하다.  
스토리지 유형 역시 K8s 리소스로 생성된다.  
- 스토리지 유형 정의 세 가지 필드 (스토리지 동작 지정)
  1. provisioner
     - PV를 만드는 주체
     - 플랫폼마다 관리 주체가 다르다. (AKS는 애저 파일스가 스토리지를 만든다.)
  2. reclaimPolcy
     - PVC가 삭제되었을 때 PV 처리 방향을 지정한다. -> 함께 삭제 또는 남겨둔다
  3. volumeBindingMode
     - 영구 볼륨 사용 방향 (둘 중 하나)
       - PVC 생성시 바로 PV를 생성해서 연결
       - PVC가 사용하는 파드가 생성 될 때 PV 생성
  
세 가지 속성을 조합하여 클러스터에 원하는 스토리지 유형을 정의할 수 있다.  
앱은 필요한 스토리지 유형을 골라 요청후 사용하면 된다.  
- 실습 
  - 기본 스토리지 유형 복제 - 스크립트 작성
  - kubectl get storageclass
    - 클러스터에 정의된 스토리지 유형 목록 확인
  - Set-ExecutionPolicy Bypass -Scope Process -Force; ./cloneDefaultStorageClass.ps1
    - 기본 스토리지 유형 복제 (윈도우)
  - chmod +x cloneDefaultStorageClass.sh && ./cloneDefaultStorageClass.sh
    - 기본 스토리지 유형을 복제 (Mac & 리눅스)
  - kubectl get sc
    - 클러스터에 정의된 스토리지 유형 목록 다시 확인
  
클러스터마다 볼륨 관리 주체가 다르고, 실습 환경 동작을 보장 할 수 없어서 단순 기본 유형 복제를 실습했다.  
PVC 정의에서 선택할 수 있는 사용자 정의 스토리지 유형이 생겼다. (복제), 기본 스토리지 유형의 kiamol   
클라우드 플랫폼 -> 동적 볼륨 프로비저닝 훨씬 빠르고 간단  
  
스토리지 유형 관리 주체 -> 스토리지 시스템 통합 컴포넌트  
- 예제 5-9
  - yaml -> 사용자 정의 스토리지 PVC 정의
  ```yaml
  spec:
    accessModes:
      - ReadWriteOnce
    storageClassName: kiamol # 스토리지 유형 -> 스토리지의 추상이라 볼 수 있다.
    resources:
      requests:
        storage: 100Mi
  ```
  
- 실습 
  - 새 PVC 생성, 이 PVC 사용하도록 DB 파드 업데이트
  - kubectl apply -f storageClass/postgres-persistentVolumeClaim-storageClass.yaml
    - 사용자 정의 스토리지 유형이 사용된 PVC 생성
  - kubectl apply -f storageClass/todo-db.yaml
    - 위 PVC 사용 DB 파드 업데이트
  - kubectl get pvc
  - kubectl get pv
  - kubectl get pods -l app=todo-db
  - to-do 앱 목록 페이지 새로고침
  
새로 생성한 PV 연결 -> 이전 데이터 모두 잃는다.  

# 5.5 스토리지를 선택할 때 고려할 점
스토리지 솔루션은 비용이 많이 든다.  
파드가 삭제돼도 볼륨 유지 비용이 청구 될 수 있으니 조심하자.  
  
DB처럼 영구 저장 용도도 K8s에서 실행해야 될까?  
데이터 관리는 K8s에서도 쉽지 않다.  
고가용성은 확보 할 수 있지만, 굳이 잘 쓰고 있는데 이전 할 필요는 없다.  
- 실습 
  - 매니페스트로 만든 모든 리소스 삭제
  - kubectl delete -f pi/v1 -f sleep/ -f storageClass/ -f todo-list/web -f todo-list/postgres -f todo-list/
    - 리소스 모두 삭제
  - kubectl delete sc kiamol

# 5.6 연습 문제
- ch05/lab/todo-list 디렉터리 내 앱 매니페스트 배치부터 시작
  - 프록시, 웹 앱 디플로이먼트 정의돼 있다.
- 정답 참조: ch05/lab/README.md
- 



# 해당 파트 사용 명령어 모음
- kubectl apply -f sleep/sleep.yaml
- kubectl exec deploy/sleep -- sh -c 'echo ch05 > /file.txt; ls /*.txt'
- kubectl get pod -l app=sleep -o jsonpath='{.items[0].status.containerStatuses[0].containerID}'
- kubectl exec -it deploy/sleep -- killall5
- kubectl get pod -l app=sleep -o jsonpath='{.items[0].status.containerStatuses[0].containerID}'
- kubectl exec deploy/sleep -- ls /*.txt
- kubectl apply -f pi/v1/
- kubectl wait --for=condition=Ready pod -l app=pi-web
- kubectl get svc pi-proxy -o jsonpath='http://{.status.loadBalancer.ingress[0].*}:8080/?dp=30000'
- kubectl exec deploy/pi-proxy -- ls -l /data/nginx/cache
- kubectl delete pod -l app=pi-proxy
- kubectl exec deploy/pi-proxy -- ls -l /data/nginx/cache
- kubectl apply -f pi/nginx-with-hostPath.yaml
- kubectl exec deploy/pi-proxy -- ls -l /data/nginx/cache
- kubectl delete pod -l app=pi-proxy
- kubectl exec deploy/pi-proxy -- ls -l /data/nginx/cache
- kubectl apply -f sleep/sleep-with-hostPath.yaml
- kubectl exec deploy/sleep -- ls -l /var/log
- kubectl exec deploy/sleep -- ls -l /node-root/var/log
- kubectl exec deploy/sleep -- whoami
- kubectl apply -f sleep/sleep-with-hostPath-subPath.yaml
- kubectl exec deploy/sleep -- sh -c 'ls /pod-logs | grep _pi-'
- kubectl exec deploy/sleep -- sh -c 'ls /container-logs | grep nginx'
- kubectl label node $(kubectl get nodes -o jsonpath='{.items[0].metadata.name}') kiamol=ch05
- kubectl get nodes -l kiamol=ch05
- kubectl apply -f todo-list/persistentVolume.yaml
- kubectl get pv
- kubectl apply -f todo-list/postgres-persistentVolumeClaim.yaml
- kubectl get pvc
- kubectl get pv
- kubectl apply -f todo-list/postgres-persistentVolumeClaim-too-big.yaml
- kubectl get pvc
- kubectl apply -f sleep/sleep-with-hostPath.yaml
- kubectl wait --for=condition=Ready pod -l app=sleep
- kubectl exec deploy/sleep -- mkdir -p /node-root/volumes/pv01
- kubectl apply -f todo-list/postgres/
- sleep 30
- kubectl logs -l app=todo-db --tail 1
- kubectl exec deploy/sleep -- sh -c 'ls -l /node-root/volumes/pv01 | grep wal'
- kubectl apply -f todo-list/web/
- kubectl wait --for=condition=Ready pod -l app=todo-web
- kubectl get svc todo-web -o jsonpath='http://{.status.loadBalancer.ingress[0].*}:8081/new'
- kubectl delete pod -l app=todo-db
- kubectl exec deploy/sleep -- ls -l /node-root/volumes/pv01/pg_wal
- kubectl apply -f todo-list/postgres-persistentVolumeClaim-dynamic.yaml
- kubectl get pvc;
- kubectl get pv;
- kubectl delete pvc postgres-pvc-dynamic
- kubectl get pv;
- kubectl get storageclass
- Set-ExecutionPolicy Bypass -Scope Process -Force; ./cloneDefaultStorageClass.ps1
- chmod +x cloneDefaultStorageClass.sh && ./cloneDefaultStorageClass.sh
- kubectl get sc
- kubectl apply -f storageClass/postgres-persistentVolumeClaim-storageClass.yaml
- kubectl apply -f storageClass/todo-db.yaml
- kubectl get pvc
- kubectl get pv
- kubectl get pods -l app=todo-db
- kubectl delete -f pi/v1 -f sleep/ -f storageClass/ -f todo-list/web -f todo-list/postgres -f todo-list/
- kubectl delete sc kiamol