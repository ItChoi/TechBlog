# 목차
- 3파트 운영 환경으로 가자
    - 12.1 정상 파드에만 트래픽 라우팅하기: 레디니스 프로브
    - 12.2 고장을 일으킨 파드 재시작하기: 리브니스 프로브
    - 12.3 헬름을 이용한 안전한 애플리케이션 업데이트
    - 12.4 계산 리소스를 관리하여 애플리케이션 및 노드 보호하기
    - 12.5 자기수복형 애플리케이션의 한계점
    - 12.6 연습 문제

---

# 서론
K8s 두 계층의 추상화로 앱을 모델링한다.
1. 네트워킹 계층 (service)
2. 컴퓨팅 계층 (pod)
  
- 계층 추상화를 통해 가능한 것들
  - 네트워크 트래픽 제어
  - 컨테이너 생애 주기 통제
  - 앱의 일부분 고장난 부분 수복 가능
  - 클러스터 자체에서 앱의 일시적인 문제를 찾아 해결
  - 등등등
  
위와 같이 가능한 앱을 자기수복형 앱이라고 한다.  
컨테이너 프로브를 활용해 앱 정상 여부를 확인하고,  
리소스 사용 한계 설정을 통해 자기수복형 앱 구성하는 방법을 다뤄보자.  
  
k8s 앱 수복 능력은 한계가 있다. 이 한계점을 알 필요가 있다.  
  
# 12.1 정상 파드에만 트래픽 라우팅하기: 레디니스 프로브
k8s는 파드 컨테이너 실행 여부는 알 수 있다.  
그러나 컨테이너 속 앱 정상 상태 여부는 알 수 없다.  
  
앱 정상 상태 기준 정의가 필요하다. 예를 들면 특정 api 호출시 200 반환을 기준으로 할 수 있다.  
K8s는 컨테이너 프로브를 통해 앱 정상 상태 여부를 판단 할 수 있다.  
  
도커 이미지는 헬스 체크 기능을 설정 할 수 있다.  
K8s는 자신의 프로브를 우선하여 도커 헬스 체크 설정을 무시한다.  
  
프로브는 파드 정의에 기술한다.  
스케줄을 정의하여 앱 특정 측면 테스트 후 정상 상태 판단 지표로 활용한다.  
  
프로브 -> 컨테이너 상태 비정상 -> K8s 조치 (자기수복)  
조치 내용은 프로브 유형에 따라 다르다.  
- 프로브 유형
  1. 레디니스 프로브
     - 네트워크 수준 조치 -> 네트워크 요청 처리 컴포넌트 라우팅 관리
     - 파드 컨테이너 비정상 상태 -> 준비 상태 제외, 서비스 활성 파드 제외
  2. 리브니스 프로브
  
서비스 리소스는 요청온 트래픽을 전달받을 엔드포인트 목록이 있다.  
이 목록은 레이블 셀렉터 기반으로 준비 상태 파드만 포함된다.  
  
디플로이먼트와 레플리카셋은 파드를 관리하지만, 준비 상태가 아닌 파드 대체 기능은 없다.  
레디니스 프로브는 일시적 과부하 문제 해결에 적합하다.  
과부화된 경우 200이 아닌 503 상태 코드를 반환 받는다면 이 파드는 서비스 제외되어 요청을 받지 않는다.  
또한 과부화 상태 파드 복귀 기회를 주는데, 이 때 정상 상태가 된다면 서비스 엔드포인트 목록에 복귀한다.  
- 실습
  - 프로브 없이 앱 오류시 현상 확인
  - cd ch12
  - kubectl apply -f numbers/
  - kubectl wait --for=condition=ContainersReady pod -l app=numbers-api
  - kubectl get endpoints numbers-api
  - kubectl get svc numbers-api -o jsonpath='http://{.status.loadBalancer.ingress[0].*}:8013' > api-url.txt
  - curl "$(cat api-url.txt)/rng"
  - curl "$(cat api-url.txt)/healthz"; curl "$(cat api-url.txt)/healthz"
  - kubectl get endpoints numbers-api

k8s를 통해 서비스 안에 두 개의 파드가 실행 됐다.  
파드 하나는 정상, 하나는 비정상이다.  
로드밸런스로 인해 랜덤으로 정상 비정상이 나오지만, 엔드포인트 목록에는 두 파드가 같이 나온다.  
  
파드 정의에 레디니스 프로브 추가시 K8s가 상태 확인 엔드포인트 사용 가능하다.  
레디니스 프로브 설정시 지정한 초마다 요청을 보낸다.  
- 실습
  - kubectl apply -f numbers/update/api-with-readiness.yaml
  - kubectl wait --for=condition=ContainersReady pod -l app=numbers-api,version=v2
  - kubectl get endpoints numbers-api
  - curl "$(curl api-url.txt)/rng"
  - sleep 10
  - kubectl get endpoints numbers-api
  
다른 수단없이 레디니스 프로브에만 의존한다면 위험한 상황이 발생할 수도 있다.  
특정 파드가 한 번 이상 증상 발생시 이상 증상이 유지된다면 정상 상태로 돌아오지 않는다.  
즉 라우팅 대상이 하나 제거된 상태를 모르고 유지 할 수도 있다.  
  
- 실습
  - kubectl get endpoints numbers-api
  - curl "$(cat api-url.txt)/rng"
  - sleep 10
  - kubectl get endpoints numbers-api
  - kubectl get pods -l app=numbers-api
  - curl "$(cat api-url.txt)/reset"
  
그런데 모든 파드가 오류 발생하여 준비 상태 파드가 0개라면?  
앱은 사용 불가능해진다.  
  
프로브는 앱의 자기 수복을 돕지만, 서비스에 포함된 모든 파드를 제거하게 될 수도 있다.
  
# 12.2 고장을 일으킨 파드 재시작하기: 리브니스 프로브
K8s는 리브니스 프로브를 통해 헬스 체크 후 파드 재시작을 할 수 있다.  
  
상태 체크 메커니즘은 리브니스, 레디니스 모두 동일하다.  
조치가 다를 뿐이다.  
리브니스 프로브 조치는 컴퓨팅 수준의 파드 재시작이다.  
즉 파드 컨테이너 재시작이다.  
  
컨테이너 재시작 vs 엔드포인트 목록 제외  
재시작은 더 적극적인 조치이자, 시점을 명확히 할 필요가 있다.  
  
레디니스를 통해 서비스에서 파드를 제거하고, 리브니스를 통해 대체 파드를 생성한다.  
즉 레디니스, 리브니스를 같이 사용 할 수 있다.  
- 실습
  - kubectl apply -f numbers/update/api-with-readiness-and-liveness.yaml
  - kubectl wait --for=condition=ContainersReady pod -l app=numbers-api,version=v3
  - kubectl get pods -l app=numbers-api -o wide
  - kubectl get endpoints numbers-api
  - curl "$(cat api-url.txt)/rng"
  - sleep 20
  - kubectl get pods -l app=numbers-api
  
지속적인 고장 발생시 재시작이 해결책이 되진 않는다.  
K8s는 파드 재시작 횟수 한계가 있다.  
  
프로브는 앱 업데이트 중에도 유용하게 쓸 수 있다.  
롤아웃은 새로 투입 파드가 준비 상태가 돼야 한다.  
만약 레디니스 프로브 상태 체크 실패시 롤아웃은 진행되지 않는다.  
  
- 실습
  - kubectl apply -f todo-list/db/ -f todo-list/web/
  - kubectl wait --for=condition=ContainersReady pod -l app=todo-web
  - kubectl get svc todo-web -o jsonpath='http://{.status.loadBalancer.ingress[0].*}:8081'
- 실습
  - kubectl apply -f todo-list/db/update/todo-db-bad-command.yaml
  - kubectl get pods -l app=todo-db --watch
  
파드는 크래시루프백오프 상태에 빠질 수 있다.  
정상 실행 불가 파드가 계속 재시작하여 클러스터 자원이 낭비되는 것을 막아준다.  
  
# 12.3 헬름을 이용한 안전한 애플리케이션 업데이트
헬름은 원자적 설치와 업그레이드를 지원한다.  
설치 및 업그레이드 실패시 자동 롤백 기능도 있다.  
  
# 12.4 계산 리소스를 관리하여 애플리케이션 및 노드 보호하기
컨테이너는 가상화된 환경이다.  
k8s는 가상 환경을 만드는 역할을 한다.  
컨테이너 환경은 메모리와 CPU가 포함된다.  

메모리와 CPU는 K8s 관리 대상이 아니다.  
파드 컨테이너는 노드 CPU, 메모리 사용 제한이 없다.  
따라서 앱 강제 종료 위험과 실행 노드의 리소스 부족을 겪을 수 있다.  
  
따라서 파드 정의에서 컨테이너가 사용할 리소스 총량을 제한한다.  
프로브, 리소스 총량 설정은 운영 환경에서 필수 설정이다.  
  
- 실습
  - helm uninstall todo-list
  - kubectl get nodes -o jsonpath='{.items[].status.allocatable.memory}'
  - kubectl apply -f memory-allocator/
  - kubectl logs -l app=memory-allocator --tail 1
- 실습
  - kubectl apply -f memory-allocator/update/memory-allocator-with-limit.yaml
  - sleep 20
  - kubectl logs -l app=memory-allocator --tail 1
  - kubectl get pods -l app=memory-allocator --watch
  
리소스 사용량을 어느 정도 선으로 제한해야될까?  
- 실습
  - kubectl delete deploy memory-allocator
  - kubectl apply -f memory-allocator/namespace-with-quota/
  - kubectl get replicaset -n kiamol-ch12-memory
  - kubectl describe replicaset -n kiamol-ch12-memory
  
CPU, 메모리 사용량 모두 컨테이너 별, 네임스페이스 별로 제한을 둘 수 있다.  
그러나 사용량 제한과 적용 방식이 다르다.  
- 실습
  - kubectl get nodes -o jsonpath='{.items[].status.allocatable.cpu}'
  - kubectl apply -f pi/
  - kubectl get svc pi-web -o jsonpath='http://{.status.loadBalancer.ingress[0].*}:8012/?dp=50000'
  - kubectl apply -f pi/update/web-with-cpu-limit.yaml
- 실습
  - kubectl delete deploy deploy pi-web
  - kubectl apply -f pi/namespace-with-quota/
  - kubectl get replicaset -n kiamol-ch12-cpu
  - kubectl get endpoints pi-web -n kiamol-ch12-cpu
  - kubectl describe replicaset -n kiamol-ch12-cpu
  
리소스 사용량 제한은 앱 자체보다 클러스터 보호 목적이 크다.  
모든 파드 정의에 리소스 사용량 제한 지정을 하는 것이 좋다.  
  
리소스 사용량 제한, 컨테이너 프로브, 원자적 업그레이드 모두 앱 중단없이 운영 가능케 한다.  
그러나 K8s가 모든 오류를 해결하지 못한다는 것을 명심해야 한다.  
  
# 12.5 자기수복형 애플리케이션의 한계점
파드는 노드 안에서 동작한다.  
파드는 노드 고장이 아닌 이상 대체되지 않고, 재시작 형태로 한다.  
따라서 컨테이너 이슈를 잘 체크해야 한다.  
멀티컨테이너로 되어 있다면, 파드 재시작시 초기화 컨테이너 재실행, 사이드카 컨테이너 교체가 된다.  
  
대부분 파드 재시작은 일시적 고장으로 문제되지 않는다.  
그러나 반복된 재시작 등의 사유로 크래시루프백오프 상태가 되면 앱 중단이 될 수 있다.  
  
극단적인 오류 상황에서 K8s에서 가능한 조치가 없는 경우 앱 중단 기간이 늘어날 수 있다.  
따라서 k8s 기능을 잘 활용해서 무중단 기간을 이어갈 수 있어야 한다.  
- 실습
  - kubectl delete ns -l kiamol=ch12
  - kubectl delete all -l kiamol=ch12
  - kubectl delete secret,configmap,pvc -l kiamol=ch12
  














































