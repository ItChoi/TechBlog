컨테이너화된 앱을 실제 운영에 투입 전에 중요한 것들을 배운다.
도커 이미지 최적화, 앱을 도커 플랫폼에 통합하는 내용이다.
여기서 말하는 통합은 앱 설정 파일을 도커에서 읽고 앱의 로그를 도커에 출력하는 것이다.

또한 리버스 프록시와 메시지 큐를 도커와 결합해 강력하고 직관적인 설계를 만들 수 있다.

# 17장 도커 이미지 최적화하기: 보안, 용량, 속도
컨테이너화된 도커 앱을 운영 환경에 투입 전 최적화를 해야 한다.
가장 중요한 것이 바로 이미지 최적화다.
최적화를 통해 이미지 빌드와 배포가 빠르게 되고, 앱의 보안이 지켜질 수 있다.

Dockerfile 문법이 간단하고 직관적이긴하다.
하지만 최선의 성능을 위해 복잡한 내막을 이해해야 한다.

## 17.1 도커 이미지를 최적화하는 방법
도커 이미지는 이미 최적화가 상당히 잘 된 포맷이다.
이미지끼리 레이어를 최대한 공유해서 이미지 빌드, 네트워크 트래픽, 디스크 사용량을 효율적으로 사용할 수 있다.
하지만 도커는 데이터를 명시적으로 삭제하지 않는 한 자동 삭제는 안 된다.
즉 앱의 업데이트를 통해 새로운 이미지를 내려 받아도, 기존 이미지가 삭제되지 않는다.
이러한 이유로 주기적 업데이트가 오히려 디스크 용량 잠식을 부르는 경우가 많다.

실습) 17.1 (1)
- docker system df
  - 내려 받은 이미지, 컨테이너과 볼륨, 빌드 캐시 등 점유 중인 실제 디스크 용량 체크 가능

도커 엔진에서 오래된 이미지를 제거한 적이 없다면, 디스크 용량을 보고 깜짝 놀랄 수 있다.
기간이 길수록 관리되지 않는다면 환장하게 된다.
따라서 주기적으로 사용하지 않는 이미지 레이어, 빌드 캐시를 비워주는 것이 좋다.
- docker system prune

실제 수동 삭제를 통해 관리하는 방법도 있지만, 이미지를 잘 최적화한다면 이 문제는 크게 줄어든다.
기술 스택 최적화 작업은 작은 개선을 지속적 반복하는 형태가 많다.
하지만 도커는 베스트 프랙티스 준수만으로 큰 폭의 개선이 가능하다.
1. 꼭 필요한 파일만 이미지에 포함시켜야 한다.
   - 실제 Dockerfile 스크립트 작성시 사용되지 않을 이미지, 문서 파일까지 한 꺼번에 디렉터리 채로 포함시키는 경우가 많다.
   - COPY 인스트럭션은 꼭 필요한 파일만 골라 복사하는 편이 낫다. 전체 복사 후 선별 삭제는 이미 전체가 특정 이미지 레이어에 포함되게 된다.
   - 도커 파일 리팩토링 서적 참고
     - 간단하게 불필요한 파일, 폴더 직접 삭제 명령어 추가로 이미지 크기가 줄어들거라 생각하면 오산이다.
     - 아래 실습을 통해 특정 파일 수동 삭제 명령어 입력, 미입력 두 개로 이미지를 빌드해도 크기는 동일한 것을 알 수 있다.

실습) 17.1 (2) 리팩토링 비교
- cd ch17/exercises/build-context
- docker image build -t diamol/ch17-build-context:v1 .
- docker image build -t diamol/ch17-build-context:v2 -f ./Dockerfile.v2 .
- docker image ls -f reference='diamol/ch17*'

Dockerfile 스크립트의 인스트럭션 하나마다 이미지 레이어가 하나씩 생긴다.
이미지 레이어가 모두 합쳐져 전체 이미지가 된다.
한 번 이미지에 복사된 파일은 이미지에서 뺼 수 없다.
복사 후 파일 또는 폴더가 삭제되더라도 파일 시스템에 숨겨질 뿐 물리 삭제가 되지 않는다.
이미지 최적화시 가장 먼저 고려해야 되는 점이다.

실습) 17.1 (3) 
- docker container run diamol/ch17-build-context:v2
 - 전체 이미지로 컨테이너 실행
- docker history diamol/ch17-build-context:v2
 - 이미지 히스토리를 통해 삭제 이전 레이어 식별자 확인
- docker container run <이전_레이어_식별자)

이미지 구조 중간에 오는 레이어로도 컨테이너 실행 가능하다.
다만 파일 시스템의 내용이 해당 레이어까지만 병합된 상태다.
    
COPY 인스트럭션을 통해 이미지 최적화를 해도 아직 최적화의 여지는 남았다.
도커 빌드 과정은 엔진에 빌드 컨텍스트 압축을 하고 도커 스크립트를 함께 보내면서 시작된다.
이 빌드 컨텍스트에 불필요한 파일이 포함된 경우가 많다.
2. .docerignore 파일을 통해 불필요 디렉터리, 파일 목록을 기재해 빌드 컨텍스트에서 제외할 수 있다.
   - .gitignore에 있는 목록들을 같이 포함시켜도 된다.

실습) 17.1 (4) .dockeringore 파일 작성, 최적화 
- docker image build -t diamol/ch17-build-context:v3 -f ./Dockerfile.v3 .
  - 컨텍스트에 불필요 파일들 존재
- mv rename.dockerignore .dockerignore
- docker image build -t diamol/ch17-build-context:v3 -f ./Dockerfile.v3 .

## 17.2 좋은 기반 이미지를 고르는 법
기반 이미지의 크기는 디스크 용량, 네트워크 전송 시간 뿐 아니라, 애플리케이션 보안과도 관계가 깊다.
크기가 크면 다양한 도구가 포함되어 있는데, 유용한지와는 별개로  컨테이너의 보안상 허점이 될 수 있다.
예를 들어 OS 기반 이미지에 curl가 설치 돼 있다면, 침입한 공격자가 이를 악용해 악성 프로그램을 컨테이너로 내려받거나, 데이터를 가로챌 수 있다.

자바 앱은 OpenJDK 공식 이미지를 기반으로 많이 사용한다.
도커 허브에서 변종 이미지 목록만 확인해도, 기반 이미지 크기가 천차만별인 것을 확인할 수 있다.

따라서 크기가 작은 기반 이미지, 변종 이미지부터 검토해야 한다.
FROM 인스트럭션에서 기반 이미지를 쉽게 바꿀 수 있으므로 여러 번 테스트하여 적합한 이미지를 찾으면 된다.

이미지에 많은 것이 포함된다면, 침입자가 다음 공격으로 이어 나갈 좋은 수단이 되기도 한다.

실습) 17.2 (1)
- cd ch17/exercises/truth-app
- docker image build -t diamol/ch17-truth-app .
- docker container run -d -p 8010:80 --name truth diamol/ch17-truth-app
- curl http://localhost:8010/truth


실습) 17.2 (2), 셸로 접속해 공격자 빙의
- docker container exec -it truth sh
- javac FileUpdateTest.java
  - 컨테이너 내부 자바 테스트 코드 컴파일, 실행
- java FileUpdateTest
- exit

특정 api의 결과 값이 바뀌었다.
심한 경우 컨테이너를 망가뜨릴 수 있다.

이미지에 방치한 불필요한 파일이 공격수단이 될 수 있다.
즉 기반 이미지는 앱 실행에 필요한 모든 것을 갖춰야 하지만, 빌드에 필요한 도구는 포함시키면 안 된다.
인터프리터 언어는 빌드 도구가 앱 실행에 사용되므로 해당되지 않는다.

골든 이미지는 이러한 문제를 피할 수 있는 한 가지 방법이다.

실습) 17.2 (3) 앤코어는 오픈 소스 도커 이미지 분석 도구다. pwd 사용해 저장소 복제 후 실습 진행
- cd ch17/exercises/anchore
- docker-compose up -d
- docker exec anchore_engine-api-1 anchore-cli system wait
- docker container cp "$(pwd)/../../../image/openjdk/Dockerfile" anchore_engine-api-1:/Dockerfile
- docker container exec anchore_engine-api-1 anchore-cli image add diamol/openjdk --dockerfile /Dockerfile
- docker container exec anchore_engine-api-1 anchore-cli image wait diamol/openjdk

wait 명령을 통해 앤코어의 사용 준비가 끝날 때 까지 터미널 세션을 막아 놓았다.
앤코어 분석이 끝나면 이미지에 포함된 다양한 문제점을 지적한다.

실습) 17.2 (4) anchore-cli wait 명령 끝난 후 체크
- docker container exec anchore_engine-api-1 anchore-cli image content diamol/openjdk java
  - 이미지에 사용된 자바 컴포넌트 확인
- docker container exec anchore_engine-api-1 anchore-cli image vuln diamol/openjdk all
  - 이미지에 발견된 취약점 체크

예제에 컴포즈 파일에 사용된 이미지들은 위협이 되지 않는다는 안내가 나와서 사용해도 무방하다.
openjdk:11-jdk 이미지 분석 결과 많은 취약점이 나오게 된다.
그 중 코어 SSL 보안 라이브러리의 '낮음' 등급 하나를 제외하면 대부분 심각도가 '불명'이다.
OpenJDK가 공식 배포한 이미지라도 만족하지 못 하는 경우 사용하지 않을 근거가 된다.

앤코어는 취약점 분석 일개 도구다.
어떤 도구를 사용하더라도 이미지의 보안상 문제점을 이해하고 신뢰성을 향상시킬 수 있다.

## 17.3 이미지 레이어 수와 이미지 크기는 최소한으로
앱 이미지 최적화 전제 조건은 최소한의 크기, 보안성을 갖추는 것이다.
그 후 꼭 필요한 것만 포함하여 이미지를 만든다.
심플하지만 쉽지 않다.
대부분 불필요 요소나 설치 후 잔재가 발생한다.
따라서 이런 요소도 확실히 통제, 제어 해야 한다.

실습) 17.3 (1) 패키지 목록 제거를 통해 정리하면 이미지 크기를 얼마나 줄일 수 있는지 체크해본다
- cd ch17/exercises/socat
- docker image build -t diamol/ch17-socat:v1 .
- docker image build -t diamol/ch17-socat:v2 -f Dockerfile.v2 .

두 이미지는 기능적으로 동일하다
그러나 한 이미지가 20MB 가량 더 적다.
패키지 설치 명령에 몇 가지 조정
1. apt 추천 패키지 설치하지 않는 옵션 사용
2. 설치 후 패키지 목록의 캐시 삭제를 하나의 RUN 인스트럭션으로 합쳤다.

여러 개의 RUN 인스트럭션을 하나로 합치는데는 또 다른 장점도 있다.
- 이미지 레이어 수를 줄이는 것은 최적화가 아니지만, 최대 레이어 수가 제한 돼 있다. (보통 127개)
  - 여분 레이어 남기는 것이 유의미
  - 레이어가 적으면 컨테이너 파일 시스템 내용 추적이 수월

실습) 17.3 (2)
- cd ch17/exercises/ml-dataset
- docker image build -t diamol/ch17-ml-dataset:v1 .
- docker image build -t diamol/ch17-ml-dataset:v2 -f Dockerfile.v2 .
- docker image ls -f reference=diamol/ch17-ml-dataset

디스크 용량이 가장 절약되는 부분 중 하나는 필요한 파일만 압축 해제하는 것이다.
위에 실습에서 v1, v2의 크기 차이는 엄청나다.

개발 업무 편의를 유지하면서 이미지 최적화 방법이 또 있다.
파일을 다루는 단계를 모두 스테이지로 분리해 디스크 용량 절약하는 멀티 스테이지 빌드다.

## 17.4 멀티 스테이지 빌드를 한 단계 업그레이드하기
멀티 스테이지는 최종 결과 이미지를 최적화하기 유리하다.

```yaml
# 스크립트 가독성과 이미지 치적화를 모두 고려한 멀티 스테이지 스크립트
FROM diamol/base AS download
ARG DATASET_URL=https://archive.ics.uci.edu/.../url_svmlight.tar.gz
RUN wget -0 dataset.tar.gz ${DATASET_URL}

FROM diamol/base AS expand
COPY --from=download dataset.tar.gz .
RUN tar xvzf dataset.tar.gz

FROM diamol/base
WORKDIR /dataset/url_svmlight
COPY --from=expand url_svmlight/Day1.svm .
```

실습) 17.4 (1) target 값 지정을 통한 멀티 스테이지 빌드 중단
- cd ch17/exercises/ml-dataset
- docker image build -t diamol/ch17-ml-dataset:v3 -f Dockerfile.v3 .
  - 버전 이미지 끝까지 빌드
- docker image build -t diamol/ch17-ml-dataset:v3-download -f Dockerfile.v3 --target download .
  - download 스테이지까지만 빌드한다 - Dockerfile은 같고 태그가 달라진다.
- docker image build -t diamol/ch17-ml-dataset:v3-expand -f Dockerfile.v3 --target expand .
  - expand 스테이지까지만 빌드
- docker image ls -f reference='diamol/ch17-ml-dataset:v3*'
  - 이미지 크기 확인

중간 단계에서 끊긴 이미지는 오히려 이미지 크기가 상당히 크다.
최종 단계 이미지 빌드는 오히려 작은 이미지 크기로 보아, 최적화된 이미지라 볼 수 있다.

실습) 17.4 (2) 최소 요소로 젠킨스 이미지 설치
- cd ch17/exercises/jenkins
- docker image build -t diamol/ch17-jenkins:v1 .
- docker image build -t diamol/ch17-jenkins:v2 -f Dockerfile.v2 .
- echo 2.0 > jenkins.install.UpgradeWizard.state
- docker image build -t diamol/ch17-jenkins:v1 .
- docker image build -t diamol/ch17-jenkins:v2 -f Dockerfile.v2 .

도커파일 내 스크립트의 캐시를 잘 활용하면 소스 코드 수정으로 CI/CD 파이프라인에서 시간 낭비하지 않고도 이미지를 빌드, 푸시 할 수 있다.
그러나 RUN 인스트럭션을 사용해 내려받거나 설치한 다른 SW처럼 불필요 요소까지 캐싱하지 않도록 주의해야 한다.

## 17.5 최적화가 중요한 이유
Dockerfile 스크립트를 위한 베스트 프랙티스 정리
1. 기반 이미지 잘 고르기, 자신만의 골든 이미지 갖출 수 있다면 이상적이다.
2. 아주 간단한 앱이 아니라면 멀티 스테이지 빌드 적용
3. 불필요 패키지, 파일 포함하지 말고, 레이어 크기를 최소한으로 유지
4. Dockerfile 스크립트의 인스트럭션은 자주 수정되는 것을 뒤에 오도록 배치해 캐시를 최대한 활용

## 17.6 연습 문제
- 서적 참고

- docker image build -t diamol/ch17-test-linux ./linux
- docker image build -t diamol/ch17-test-window ./windows


---

# 18장 컨테이너의 애플리케이션 설정 관리
애플리케이션은 환경에 따른 설정을 외부로부터 주입받아야 한다.
설정은 주로 환경 변수 또는 파일 형태를 갖는다.
도커는 컨테이너에서 실행되는 앱 환경을 만들어 주고, 환경 변수를 설정하고 파일 시스템을 구성한다.
여러 곳에 분리된 설정 값을 잘 활용해 병합함으로써 환경 설정을 구성하면 된다.

## 18.1 다단 애플리케이션 설정
설정 모델은 설정에 담긴 데이터의 구조를 반영해야 한다.
설정 데이터 종류 세 가지
1. 버전에 따라 달라지는 설정
2. 환경에 따라 달라지는 설정
3. 기능 설정: 버전 별로 앱 동작을 달리하기 위한 설정

config: 기본 설정으로 도커 이미지에 포함되는 설정
config-override: 이미지에 미포함이지만 볼륨, 컨피그 객체, 비밀 값 등을 통해 컨테이너 파일 시스템에 주입되는 과정

실습) 18.1 (1) 앱 기본 설정으로 실행, 오버라이드 후 실행
- cd ch18/exercises/access-log
- docker container run -d -p 8080:80 diamol/ch18-access-log
- docker container run -d -p 8081:80 -v "$(pwd)/config/dev:/app/config-override" diamol/ch18-access-log
- curl http://localhost:8080/config
- curl http://localhost:8081/config

실습) 18.1 (2) 
- cd ch18/exercises/access-log
- docker container run -d -p 8082:80 -v "$(pwd)/config/dev:/app/config-override" -e NODE_CONFIG='{\"metrics\": {\"enabled\":\"true\"}}' diamol/ch18-access-log
- curl http://localhost:8082/config

프로메테우스 정보 수집 비활성화를 통해 CPU와 메모리 자원 절약 가능하다.
따라서 비활성화로 했다가, 환경 변수 값 수정을 통해 활성화하여 정보 수집을 할 수 있다.

이런 방식이 모든 앱에 기본적으로 적용되는 핵심 패턴이다.
하지만 이 패턴은 실수 발생 여지가 있다.
따라서 유연성을 조금 희생하고 위험을 해결할 방법이 있다.

## 18.2 환경별 설정 패키징하기
환경 변수, 설정 파일을 배포에 포함시킬 수 있는 다양한 방법을 제공한다.

닷넷 코어는 두 파일로부터 기본 설정 값을 읽어들인다.
1. appsettings.json:
2. appsettings.{환경_이름}.json:
3. 환경 변수: 환경 이름 정의 및 오버라이드 설정

실습) 18.2 (1) 기본 설정 앱 실행, 테스트 환경 설정 재실행
- docker container run -d -p 8083:80 diamol/ch18-todo-list
- docker container run -d -p 8084:80 -e DOTNET_ENVIRONMENT=Test diamol/ch18-todo-list

동일한 이미지를 사용해 앱을 실행했지만, 서로 다른 설정 파일이 적용됐다.
설정 파일과 소스 코드 별도 시스템으로 관리시 이런 방법도 유용하다.
레지스트리는 항상 외보 노출 위험이 있다고 가정하고 보안에 만전을 가해야 한다.
그렇지 않으면 패스워드, API 키 등 이미지를 통해 평문으로 유출 될 수도 있다.

실습) 18.2 (2) 오버라이드 설정 파일 활용
- cd ch18/exercises/todo-list
- docker container run -d -p 8085:80 -e DOTNET_ENVIRONMENT=Production -v "$(pwd)/config/prod-local:app/config-override" diamol/ch18-todolist

실습) 18.2 (3) 환경 변수 이용해 릴리스 주기 정보 오버라이드
- docker container run -d -p 8086:80 -e DOTNET_ENVIRONTMENT=Production -e release=CUSTOM -v "$(pwd)/config/prod-local:/app/config-override" diamol/ch18-todo-list

이미지에 설정 파일을 모두 포함시키는 방법이 널리 쓰이긴 하지만, 저자는 이 방식을 선호하지 않는다.
스스로 판단하기를 민감하지 않은 정보를 포함하겠지만, 보안 부서의 판단 기준은 다를 수 있기 때문이다.
서버 이름, URL, 파일 경로, 로그 수준, 캐시 크기 등 생각치도 못한 유용한 공격 정보가 될 수 있다.

저자는 설정 정보를 소스 코드 형상 관리와 설정 관리로 이원화하는 것을 선호하지 않는다.

## 18.3 런타임에서 설정 읽어 들이기
Go 언어에서 바이퍼(Viper)라는 설정 모듈이 널리 쓰인다.
패키지 목록에 해당 설정 파일을 추가하고, 오버라이드 읽어올 설정 디렉터리 지정만 하면 된다.
1. 이미지에 포함된 config 디렉터리
2. 환경 별 설정 파일은 config-override 디렉터리에서 읽어 들이고, 이미지에서 비어 있따가 파일 시스템 마운트로 외부에서 주입된다.
3. 환경 변수는 설정 파일의 설정 값을 오버라이드 할 수 있다.

go 언어는 TOML, JSON, YAML 등 활용 가능하지만 TOML이 널리 쓰인다.
```toml
release = "19.12"
environment = "UNKNOWN"

[metrics]
enabled = true

[apis]
    [apis.image]
    url = "http://iotd/image"

    [apis.access]
    url = "http://accesslog/access-log"
```

TOML은 특히 클라우드 기반 프로젝트에서 장점이 더 크기에 널리 사용된다.
가독성과 디버깅이 수월하고 병합 도구를 통해 버전별 설정 차이 체크가 쉽다.

확장자만 제외하면 앱의 설정 관리는 Node.js와 동일하다.

실습) 18.3 (1)
- docker container run -d -p 8086:80 diamol/ch18-image-gallery
- curl http://localhost:8086/config

민감한 데이터를 API를 통해 노출되면 안 되기 때문에, API 유의 사항은 다음과 같다.
1. 전체 설정을 공개하지 않는다. 민감 정보는 절대 포함시키지 마라.
2. 허가받은 사용자만 접근 가능하도록 엔드포인트에 보안 설정
3. 설정 API의 사용 여부를 설정 할 수 있도록 한다.

실습) 18.3 (2) 오버라이드 파일로 환경별 설정 구성 후 실행
- cd ch18/exercises/image-gallery
- docker container run -d -p 8087:80 -v "$(pwd)/config/dev:/app/config-override" diamol/ch18-image-gallery
- curl http://localhost:8087/config

조직에서 도커 첫 도입시 사용 범위가 급속하게 확산되는 경향이 있다.
도커를 통한 다양한 앱 실행과 설정도 다양하다.
오버라이드 설정 파일, 환경 변수 형식은 표준화하기 어렵다.

실습) 18.3 (3) 설정 오버라이드
- cd ch18/exercises/image-gallery
- docker container run -d -p 8088:80 -v "$(pwd)/config/dev:/app/config-override" -e IG_METRICS.ENABLED=TRUE diamol/ch18-image-gallery
- curl http://localhost:8088/config

## 18.4 레거시 애플리케이션에 설정 전략 적용하기
레거시 앱도 나름 기존의 설정 전략이 있다.
그러나 보통 환경 변수, 설정 파일을 통한 병합하는 것은 지원하지 않는다.

그러나 Dockerfile 스크립트를 잘 활용하면 설정 전략을 적용할 수 있다.
컨테이너에 주입된 설정 파일을 앱 설정 전략에 맞춰 변환하는 유틸리티 또는 스크립트를 이미지에 포함시키는 방법이다.
1. 컨테이너에 지정된 오버라이드 설정 파일 읽기
2. 환경 변수에서 오버라이드 설정 읽기
3. 오버라이드 설정 파일과 환경 변수 설정 병합, 환경 변수 값이 우선이다.
4. 병합된 오버라이드 설정을 컨테이너 내 대상 설정 파일에 추가

실습) 18.4 (1) 레거시 앱 기본 설정, 오버라이드 설정 각각 적용 후 실행
- cd ch18/exercises/image-of-the-day
- docker container run -d -p 8089:80 diamol/ch18-image-of-the-day
- docker container run -d -p 8090:80 -v "$(pwd)/config/dev:/config-override" -e CONFIG_SOURCE_PATH="/config-override/application.properties" diamol/ch18-image-of-the-day
- curl http://localhost:8089/config
- curl http://localhost:8090/config

도커 이미지 확장을 통해 레거시 앱을 현대적 설정 모델을 도입 할 수 있다.
어쩔 수 없이 시간 간격이 생기고, 컨테이너 실패 여지가 생긴다.
따라서 대처 가능하도록 항시 헬스 체크를 적용해야 한다.

실습) 18.4 (2)
- docker run -d -p 8091:80 -v "$(pwd)/config/dev:/config-override" -e CONFIG_SOURCE_PATH="/config-override/application.properties" -e IOTD_ENVIRONMENT="custom" diamol/ch18-image-of-the-day
- curl http://localhost:8091/config

실습) 18.4 (3)
- docker container rm -f $(docker container ls -aq)
- cd ch18/exercises
- docker-compose up -d
- curl http://localhost:8030/config
- curl http://localhost:8020/config
- curl http://localhost:8010/config

## 18.5 유연한 설정 모델의 이점
실무에서 버전에 따라 다른 설정은 이미지에 포함시키고, 
환경 별로 다른 설정은 컨테이너 플랫폼에서 제공하는 오버라이드 파일을 통해 적용하고,
환경 변수를 통해 통제하는 기능별 설정을 덧붙일 수 있다.

이러한 설정 모델을 통해 운영 환경 이슈에 기민하게 대응 가능하다.
설정 모델에 대한 시간 투자는 동일 이미지로 모든 환경의 앱을 동작시키는 결과로 보답 받는다.

유연한 설정을 위해서는 책에 나온 내용 외에도 많다.
모든 컨테이너 런타임은 컨피그 객체, 비밀 값, 환경 변수 설정 기능을 갖추고 있다.

## 18.6 연습 문제

---

# 19장 도커를 이용한 로그 생성 및 관리
도커 로그는 앱 로그가 표준 출력 스트림으로 출력되고 있는지만 확인하면 된다.
도커는 플러그인 로깅 프레임워크를 갖췄다.
컨테이너에서 출력되는 로그를 도커가 원하는 곳으로 전달해준다.
이를 활용해 모든 로그를 중앙 로그 저장소에 저장 후 수집된 로그를 검색할 수 있다.


## 19.1 표준 에러 스트림과 표준 출력 스트림
도커 이미지는 앱의 바이너리, 의존성, 컨테이너 시작시 실행 행위 등을 담은 파일 시스템의 스냅샷이다.

컨테이너 시작시 실행 프로세스는 포어그라운드로 동작한다.
실행 프로세스에서 생성한 로그 엔트리는 표준 출력(stdout), 표준 오류(stderr) 스트림으로 출력된다.
터미널에서 앱 출력 내용 확인 가능한 것이 바로 이 덕분이다.
도커는 각 컨테이너의 표준 출력, 오류 등을 주시하며 스트림을 통해 출력되는 내용을 수집한다.

실습) 19.1 (1) 앱 로그 출력
- docker container run diamol/ch15-timecheck:3.0

도커가 컨테이너 안에서 프로세스를 실행하고, 프로세스가 표준 스트림으로 출력한 내용을 로그로 수집한다.
이런 형식이 표준적인 컨테이너 운영 모델이다.
런타임이 앱 로그를 출력 스트림으로 내보내면 도커가 이를 수집한다.

json 파일로도 특정 로그를 수집한다.
이 파일은 도커가 직접 컨테이너와 동일한 생애주기를 갖도록 관리한다.
즉 컨테이너 제거시 로그 파일도 삭제된다.

실습) 19.1 (2) 터미널 세션과 분리된 백그라운드 실행
- docker container run -d --name timecheck diamol/ch15-timecheck:3.0
- docker container logs --tail 1 timecheck
- docker container stop timecheck
- docker container logs --tail 1 timecheck
- docker container inspect --format='{{.LogPath}}' timecheck

실습) 19.1 (3) 앱 로그 옵션 변경 후 실행
- docker container run -d --name timecheck2 --log-opt max-size=5k --log-opt max-file=3 -e Timer__IntervalSeconds=1 diamol/ch15-timecheck:3.0
  - 세 개의 파일로 로그 로테이트 적용, 5KB 도달시 다음 파일로 넘어간다.
- docker container inspect --format='{{.LogPath}}' timecheck2

로그 파일 경로에 json 파일 하나만 나오지만, 로그 파일명에 접미사를 붙이며 새로운 로그 파일이 업데이트 적용이 되고 있다.

## 19.2 다른 곳으로 출력된 로그를 stdout 스트림에 전달하기
표준 로그 모델 적용이 어려운 앱도 있다.
아무 내용도 출력하지 않는 앱이 그렇다.

실습) 19.2 (1) 
- docker container run -d --name timecheck3 diamol/ch19-timecheck:4.0
- docker container logs timecheck3
  - 아무 로그도 출력되지 않는다.
- docker container exec -it timecheck3 sh
- cat /logs/timecheck.log
  - 로그를 수집하고 있는 것을 확인 ㅏ능

도커에서 로그 수집이 되지 않는 이유는 해당 앱이 표준 출력 스트림이 아닌 채널을 사용해 로그를 출력한다.
도커는 표준 출력 스트림을 통해서만 로그를 수집한다.

이런 형식은 로그 파일의 내용을 읽고 표준 출력으로 내보내 주는 별도 프로세스를 컨테이너 시작 명령을 통해 실행한다.
이 방법은 단점이 있다. 로그 전달 유틸리티는 포어그라운드로 동작한다.
따라서 프로세스 종료시 앱, 컨테이너 모두 종료되기 때문에 오류에 주의해야 한다.
단점에도 불구하고 꽤 유용한 패턴이다.

```yaml
# 로그 전달 유틸리티 사용 앱 빌드
FROM diamol/dotnet-runtime AS base
WORKDIR /app
COPY --from=builder /out/ .
COPY --from=utility /out/ .

FROM base AS linux
CMD dotnet TimeCheck.dll & dotnet Tail.dll /logs timecheck.log
```
실습) 19.2 (2)
- docker container run -d --name timecheck4 diamol/ch19-timecheck:5.0
- docker container logs timecheck4
- docker container exec -it timecheck4 sh
- cat /logs/timecheck.log

이 방식의 단점은 로그를 전달 하는 과정, 로그가 두 번 저장된다.
따라서 연산 능력과 디스크 용량이 낭비된다.

모든 이미지에서 컨테이너 로그 수집 준비가 되면, 
수집된 로그들을 종합적으로 도커의 플러그인 로깅 시스템을 적용해보자.

## 19.3 컨테이너 로그 수집 및 포워딩하기
가장 널리 쓰이는 오픈 소스 로깅 시스템인 fluentd를 예제로 살펴보자.
fluentd는 통합 로깅 계층이다.
로그들을 수집하고, 필터링, 가공을 통해 포워딩 역할을 한다.

실습) 19.3 (1)
- cd ch19/exercises/fluentd
- docker container run -d -p 24224:24224 --name fluentd -v "$(pwd)/conf:/fluentd/etc" -e FLUENTD_CONF=stdout.conf diamol/fluentd
- docker container run -d --log-driver=fluentd --name timecheck5 diamol/ch19-timecheck:5.0
- docker container logs timecheck5
- docker container logs --tail 1 fluentd 

fluentd는 수집한 로그에 자체 메타데이터(컨테이너 아이디, 이름 등)를 추가해 저장한다.
전체 앱을 대상으로 로그를 수집하기 때문에 로그 컨텍스트 정보 파악을 위해 메타데이터가 필요하다.

수집된 로그는 대게 중앙 데이터 스토어로 전송된다.
일래스틱서치 등이 로그 데이터 스토어로 널리 쓰인다.
일래스틱서치에 로그를 저장하고 키바나를 함께 사용하는 것이 일반적이다.

실습) 19.3 (2)
- docker container rm -f $(docker container ls -q)
- cd ch19/exercises
- docker-compose -f fluentd/docker-compose.yml up -d
- docker container run -d --log-driver=fluentd diamol/ch19-timecheck:5.0

실습) 19.3 (3) fluentd 적용 예제
- ch19/exercises
- docker-compose -f image-gallery/docker-compose.yml up -d

```yaml
# fluentd에서 로그의 출처를 알 수 있는 태그 추가 설정
services:
  accesslog:
    image: diamol/ch18-access-log
    logging:
      driver: "fluentd"
      options:
        tag: "gallery.access-log.{{.ImageName}}"
        
  iotd:
    image: diamol/ch18-image-of-the-day
    logging:
      driver: "fluentd"
      options:
        tag: "gallery.iotd.{{ImageName}}"

  image-gallery:
    image: diamol/ch18-image-gallery
    logging:
      driver: "fluentd"
      options:
        tag: "gallery.image-gallery.{{.ImageName}}"
```

도커에는 다양한 로깅 드라이버 사용 가능하다.
로깅 시스템 경험이 많지 않다면 fluentd도 추천한다.

## 19.4 로그 출력 및 로그 컬렉션 관리하기
로그는 대량 데이터 저장, 문제 진단에 필요한 정보 확보 사이를 오가는 줄타기와 같다.
대량 데이터 저장은 자칫 불필요한 용량 낭비를 초래한다.
도커의 유연한 로깅 모델 이용시, 컨테이너의 상세한 로그를 생산하면서도 필터를 적용해 로그를 저장할 수 있다.
즉, 불필요 용량 낭비를 그나마 커스텀 가능하다.

실습) 19.4 (1)
- docker-compose -f fluentd/docker-compose.yml -f fluentd/override-gallery-filtered.yml up -d
- docker-compose -f image-gallery/docker-compose.yml -f image-gallery/override-logging.yml up -d
- docker container logs --tail 1 fluentd_fluentd_1
- docker-compose -f fluentd/docker-compose.yml -f fluentd/override-gallery.yml up -d

이 방법은 일부 로그 유실 가능성이 있다.
fluentd 새 설정 재 배포 동안 컨테이너 생성 로그를 수집하지 못한다.
문제가 될 확률은 높지 않지만, 더 상세 로그 출력하도록 재 배포를 하는 것이 좋다.

## 19.5 컨테이너의 로깅 모델
도커 로깅 모델은 매우 유연성이 뛰어나다.
그러나 이 유연성은 앱 로그를 컨테이너 로그로 내보낼 때만 적용된다.

조직에 따라 이런 로깅 모델의 선호도가 다를 수 있다.
즉, 로그를 컨테이너 로그와 fluentd 컨테이너를 거치지 않고 바로 최종 저장소인 일래스틱서치에 저장하는 것을 선호한다.
저자는 이 방식을 선호하지 않는다.
모든 유연성을 희생해 얻은 대가가 약간의 처리 시간과 네트워크 트래픽 절약이다.
그리고 특정 로깅 기술에 의존하게 된다.
항상 단순하고 유연한 방법을 선호하자.

## 19.6 연습 문제


---

# 20장 리버스 프록시를 이용해 컨테이너 HTTP 트래픽 제어하기
외부 트래픽을 컨테이너까지 이어주는 라우팅은 도커 엔진이 담당한다.
하지만 컨테이너가 주시할 수 있는 네트워크 포트는 하나뿐이다.

클러스터 하나에서 수많은 앱을 실행해야 하고, 
앱들은 HTTP, HTTPS를 통해 외부 네트워크에서 접근 가능하도록 해야 한다.
리버스 프록시는 이런 경우 유용하다.
그 중 가장 널리 쓰이는 엔진엑스(Nginx)와 트래픽(Traefik)을 사례로 삼는다. 
모두 컨테이너로 동작시킬 것이다.

## 20.1 리버스 프록시란?
네트워크 프록시는 네트워크 트래픽을 처리하는 네트워크 구성요소이다.
예를 들면 회사 네트워크에서 웹 브라우저에서 전달되는 요청을 가로채 허용된 사이트인지 체크 또는 캐시를 통한 접근 등을 위해
프록시가 설치돼 있는 곳도 있다.

리버스 프록시의 역할도 이와 유사하다.
여러 웹 앱을 통하는 관물 역할을 한다.
모든 트래픽은 리버스 프록시를 거치며 해당 트래픽이 어떤 앱에서 출발 됐는지 판단한다.
또한 앱 응답 내용 캐시 후 적절히 가공해 클라에게 전달하기도 한다.

리버스 프록시도 컨테이너로 실행된다.
외부의 모든 트래픽은 리버스 프록시를 먼저 거친다.
즉, 앱 컨테이너 자체는 외부에서 접근할 수 없고 리버스 프록시를 거친다.
따라서 앱 컨테이너는 포트를 외부에 공개하지 않아도 된다.

리버스 프록시는 포트를 외부로 공개한 유일한 컨테이너다.
외부 요청을 받아 적절한 컨테이너로부터 응답을 받아 온다.
리버스 프록시 덕분에 모든 앱 컨테이너는 외부에 노출되지 않아도 된다.
따라서 스케일링, 업데이트, 보안 면에서 유리하다.

엔진엑스는 인터넷의 30% 정도를 차지 할 정도로 리버스 프록시르 널리 쓰였다.

실습) 20.1 (1) 
- docker network create ch20
- cd ch20/exercises
- docker-compose -f nginx/docker-compose.yml -f nginx/override-linux.yml up -d
    - 에러가 난다. 파일 시스템 구축이 잘못되어 있다.
    
아직 위에서 실행한 nginx 컨테이너는 리버스 프록시 기능을 하진 않는다.
리버스 프록시 사용을 위해선 사이트별 설정 파일을 하나 추가해야 한다.
같은 포트를 통해 여러 앱을 호스팅 하려면 구별 가능해야 하는데, 대게 도메인이 역할을 한다.
naver.com로 접속 시도시 웹 브라우저 HTTP 요청 헤더에 Host=naver.com 정보가 들어간다.
nginx는 헤더의 호스트 정보로 해당 요청을 처리할 사이트의 설정 파일을 찾는다.

실습) 20.1 (2)
- echo $'\n127.0.0.1 whoami.local' | sudo tee -a /etc/hosts
  - hosts 파일에 도메인 추가 
- docker-compose -f whoami/docker-compose.yml up -d
- cp ./nginx/sites-available/whoami.local ./nginx/sites-enabled/
  - 앱 설정 파일을 엔진엑스 설정 파일 디렉토리로 복사
  - whoami.local 요청을 who-am-i 컨테이너로 전달하는 설정이 있다.
- docker-compose -f nginx/docker-compose.yml restart nginx
- http://whoami.local 접속 

리버스 프록시는 웹 사이트만을 대상으로 하진 않는다.
HTTP로 제공되는 컨텐츠는 모두 대상이 된다.

요청마다 처리 컨테이너(업스트림)를 호출하고 응답한다.
업스트림에 이상시 엔진엑스는 실패 응답을 다운 스트림에 전달한다. (?)

실습) 20.1 (3)
- echo $'\n127.0.0.1 api.numbers.local' | sudo tee -a /etc/hosts
- docker-compose -f numbers/docker-compose.yml up -d
- cp ./nginx/sites-available/api.numbers.local ./nginx/sites-enabled/
- docker-compose -f nginx/docker-compose.yml restart nginx

nginx를 경유 하나 안하나 앱은 똑같이 사용 가능하다.
업스트림 컨테이너에 다다르는 라우팅을 엔진엑스가 담당한다.
엔진엑스는 트래픽의 내용을 변조하진 않는다.

리버스 프록시 기능은 이보다 훨씬 강력하다.
모든 앱의 트래픽은 프록시를 경유하므로 설정의 중심 역할을 할 수 있다.
인프라스트럭처 수준의 사항을 앱 컨테이너와 분리할 수 있다는 것도 장점이다.

## 20.2 리버스 프록시의 라우팅과 SSL 적용하기
위에서 다음과 같은 순서로 리버스 프록시에 새로운 앱을 추가했다.
1. 앱 컨테이너 시작
2. 사이트 설정 파일을 엔진엑스 컨테이너로 복사
3. 엔진엑스 컨테이너 재시작

이 순서가 중요한 이유는 엔진엑스 재시작 시점에 사이트별 설정 파일을 모두 읽고, 해당 설정의 업스트림이 모두 접근 가능한지 확인을 거친다.
업스트림 중 하나라도 접근 불가능하다면 엔진엑스는 종료된다.
업스트림 모두 접근 가능이라면, 그 다음 호스트명과 IP 주소를 연결한 내부 라우팅 리스트를 만든다.
업스트림 컨테이너가 여러 개 존재한다면 로드밸런싱까지 처리해준다.

실습) 20.2 (1)
- echo $'\n127.0.0.1 image-gallery.local' | sudo tee -a /etc/hosts
- docker-compose -f ./image-gallery/docker-compose.yml up -d --scale image-gallery=3
- cp ./nginx/sites-available/image-gallery.local ./nginx/sites-enabled/
- docker-compose -f ./nginx/docker-compose.yml restart nginx
- curl -i --head http://image-gallery.local

호스트명을 통해 엔진엑스가 컨테이너를 구별하도록 해봤다.
엔진엑스 라우팅 기능 이용시 훠맀ㄴ 더 세세한 설정 가능하다.

실습) 20.2 (2)
- rm ./nginx/sites-enabled/image-gallery.local
- cp ./nginx/sites-available/image-gallery-2.local ./nginx/sites-enabled/image-gallery.local
- docker-compose -f ./nginx/docker-compose.yml restart nginx
- curl -i http://image-gallery.local/api/image

로드 밸런싱과 라우팅까지 갖추면 단일 컴퓨터로 운영 환경과 비슷한 조건을 만들 수 있다.

SSL 종료 프록시도 있다.
앱이 HTTPS 사이트로 돼 있다면 이를 위한 설정과 인증서가 어딘가에 위치해야 한다.
이런 것들을 앱 컴포넌트마다 따로 두는 것 보다 중앙 프록시에 두고 관리하는 것이 훨씬 좋다.

실습) 20.2 (3) SSL 인증서 생성, 엔진엑스 HTTPS 프록시 적용
- docker container run -v "$(pwd)/nginx/certs:/certs" -e HOST_NAME=image-gallery.local diamol/cert-generator
- rm ./nginx/sites-enabled/image-gallery.local
- cp ./nginx/sites-available/image-gallery-3.local ./nginx/sites-enabled/image-gallery.local
- docker-compose -f nginx/docker-compose.yml restart nginx

첫 실행 컨테이너는 OpenSSL 도구를 사용해 자체 서명 인증서를 생성하고, 로컬 저장소 certs 디렉토리로 복사한다.
certs 디렉토리는 엔진엑스 컨테이너에 바인드 마운트된다.
image-gallery 앱 설정 파일을 SSL 적용 파일로 바꾸고 엔진엑스를 재시작한다.

엔진엑스는 프로토콜로부터 암호화까지 상세한 SSL 설정이 가능하다.
더 상세한 설정은 실제 적용할 때 알아보자.

인증서와 키 파일은 민감한 정보라서 운영 환경에서는 비밀 값 형태로 클러스터에 저장된다.

## 20.3 프록시를 이용한 성능 및 신뢰성 개선
엔진엑스는 고성능 HTTP 서버다.
정적 콘텐츠 제공에 활용할 수도 있다.

컨테이너 하나만으로도 초당 수천 건의 요청 처리가 가능하다.
엔진엑스를 캐싱 프록시로 사용해 고성능 앱 개선에 활용할 수 있다.
업스트림 앱에서 받아온 콘텐츠를 로컬 디스크, 메모리에 저장해 두었다가 같은 요청시 캐시를 사용한다.
즉 캐시에 요청과 일치하는 응답이 없으면 업스트림에서 콘텐츠를 받아 캐시에 저장하고, 이후 이를 사용한다.
(?) 여러 컨테이너의 로드밸런싱을 담당할 때, 모든 컨테이너의 컨텐츠를 캐싱하기엔 너무 부담되지 않나?

캐싱 프록시 장점
1. 요청 처리 시간 감소
2. 앱 트래픽 감소 - 인프라스트럭처로 더 많은 요청 처리 가능해짐

인증 쿠키 포함 요청은 캐싱하지 않도록 제외 가능하다.

실습) 20.3 (1)
- rm ./nginx/sites-enabled/image-gallery.local
- cp ./nginx/sites-available/image-gallery-4.local ./nginx/sites-enabled/image-gallery.local
- docker-compose -f ./nginx/docker-compose.yml restart nginx
- curl -i --head --insecure https://image-gallery.local
- curl -i --head --insecure https://image-gallery.local

X-Cache 헤더를 통해 캐시 MISS, HIT를 파악 가능하다.
캐시 사용에 대한 세세한 설정도 가능하다.
API 캐시는 단기 캐시로 설정돼 1분 지나면 무효화된다.
이러한 전략을 통해 로드밸런싱이 많은 컨테이너를 담당해도 부하 없도록 설정 할 수 있는 것 같다.

캐시 설정 중 업스트림(컨테이너) 사용 불가시 유효 시간이 만료된 캐시도 사용하라고 할 수 있다.

실습) 20.3 (2)
- curl -s --insecure https://image-gallery.local
- curl -s --insecure https://image-gallery.local/api/image
- docker container rm -f $(docker container ls -f name="image-gallery_image-gallery_*" -q)
- curl -i --head --insecure https://image-gallery.local
- docker container rm -f image-gallery_iotd_1
- curl -i --head --insecure https://image-gallery.local/api/image

캐시가 유효한 동안 웹 컨테이너 없이 컨텐츠가 응답된다.
엔진엑스는 리버스 프록시로도 매우 유용하지만, 캐시를 통해 성능 개선도 할 수 있다.

## 20.4 클라우드 네이티브 리버스 프록시
도커 엔진과 연결된 컨테이너는 도커 API 질의를 통해 다른 컨테이너 정보를 얻을 수 있다.
클라우드 네이티브 리버스 프록시 도구인 트래픽(Traefik)이 이런 식으로 동작한다.

트래픽 도구 사용시 앱 별로 설정 파일을 따로 둘 필요 없이 컨테이너에 레이블만 추가하면 된다.
레이블을 이용해 스스로 설정과 라우팅 맵 구성한다.
트래픽 도구 등은 동적 설정을 구성한다는 점이 장점이다.

실습) 20.4 (1)
- docker container rm -f $(docker container ls -q)
- docker-compose -f traefik/docker-compose.yml -f traefik/override-linux.yml up -d
- http://localhost:8080

트래픽 도구는 엔진엑스와 비슷한 점이 많다.
트래픽 동작 과정
- 엔트리포인트
  - 외부 트래픽 주시 포트
  - 포트와 컨테이너 공개 포트가 매핑
- 라우터
  - 요청을 배정할 컨테이너를 결정하는 규칙 (호스트명, 경로 등)
- 서비스
  - 실제 컨텐트 제공하는 업스트림 컴포넌트
- 미들웨어
  - 라우터와 서비스 사이에서 서비스에 전달되는 요청 변경 역할
  - 요청에 포함된 경로, 헤더를 변경하거나 인증을 강제 할 수 있다.

실습) 20.4 (2)
- docker-compose -f whoami/docker-compose.yml -f whoami/override-traefik.yml up -d
- http://localhost:8080/dashboard/#/http/routers/whoami@docker
- curl -i http://whoami.local

실습) 20.4 (3)
- docker-compose -f image-gallery/docker-compose.yml -f image-gallery/override-traefik.yml up -d
- curl --head http://image-gallery.local
- curl -i http://image-gallery.local

트래픽 도구의 경우 캐시를 지원하지는 않는다.
캐싱 프록시는 엔진엑스를 사용해야 한다.
반면 SSL 지원은 트래픽이 훨씬 충실하다.

실습) 20.4 (4)
- docker-compose -f image-gallery/docker-compose.yml -f image-gallery/override-traefik-ssl.yml up -d
- curl --head --insecure https://image-gallery.local
- curl --insecure https://image-gallery.local/api/image
  - --insecure 옵션을 통해 인증서 신뢰하지 못 해도 그대로 접속 진행하라는 의미다.

라우팅, 로드 밸런싱, SSL 적용은 리버스 프록시의 주요 기능이다.
트래픽 도구 사용시 이런 기능을 컨테이너 레이블을 이용해 자동 설정 적용 가능하다.
트래픽 도구는 캐시 기능이 없지만, 향후 추가 될 가능성도 있다.

엔진엑스 설정은 까다롭지만 트래픽 도구에서 쉽게 적용 가능한 기능 한 가지가 있다.
**스티키 세션(sticky session)**
현대 앱은 최대한 무상태로 만들어야 한다.
무상태를 통해 아무 컨테이너에서 요청 처리가 가능하다.
수평 확장시 성능 향상은 물론 로드 밸런싱 효과 극대화 가능하다는 중요한 특징이 있다.

앱에 상태가 있는 구성 요소를 많이 포함하고 있다면, 앱을 컨테이너 이주시키기 위해 같은 사용자 요청은 같은 컨테이너로 계속 라우팅할 필요가 생긴다.
이를 스티키 세션이라 하는데, 트래픽 도구에서 서비스 설정으로 적용 가능하다.

실습) 20.4 (5)
- docker-compose -f whoami/docker-compose.yml -f whoami/override-traefik.yml up -d --scale whoami=3
- curl -c c.txt -b c.txt http://whoami.local
- curl -c c.txt -b c.txt http://whoami.local
- docker-compose -f whoami/docker-compose.yml -f whoami/override-traefik-sticky.yml up -d --scale whoami=3
- curl -c c.txt -b c.txt http://whoami.local
- curl -c c.txt -b c.txt http://whoami.local

스티키 세션 활성화시 클라이언트에 컨테이너 식별 쿠키가 부여된다.
이를 통해 사용자 요청을 같은 컨테이너로 라우팅 할 수 있다.
쿠키에 컨테이너의 IP 주소가 삽입된 것을 확인 할 수 있다.

스티키 세션 적용시 요청이 모든 컨테이너에 고르게 로드밸런싱 하는데 영향이 생긴다.

## 20.5 리버스 프록시를 활용한 패턴의 이해
운영 환경에서 여러 앱을 운영하면 리버스 프록시는 필수적으로 도입해야 한다.
리버스 프록시가 있어야 적용 가능한 세 가지 중 패턴
1. 호스트명을 통해 HTTP, HTTPS로 제공되는 앱에서 정확한 컨텐츠 제공
리버스 프록시만 80, 443 포트를 외부로 공개하고, 연결된 컨테이너는 외부 공개 포트가 필요 없다.
리버스 프록시는 도메인 정보를 통해 컨텐츠를 받아오고 응답한다.

2. MSA에서 주로 활용
리버스 프록시는 HTTP 요청 경로를 이용해 MSA 중 일부만 선택 노출한다.
즉 외부에서는 하나의 도메인을 갖지만 경로에 따라 서로 다른 컨테이너가 요청을 처리한다.
엔드 포인트는 같은 도메인 사용, HTTP 요청 경로에 따라 다른 컨테이너로 라우팅된다.

3. 모놀리식 설계를 가진 앱을 컨테이너로 이주 할 때 활용
모놀리식 설계를 가진 프로젝트를 두고, 새로 추가되는 기능 등을 새로운 컨테이너를 통해 계속 확장시킨다.

## 20.6 연습 문제
리버스 프록시의 캐싱이 얼마나 강력한지 체크해보자.



---

# 21장 메시지 큐를 이용한 비동기 통신

---

# 22장 끝없는 정진

---
