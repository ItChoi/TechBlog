오케스트레이션은 여러 대의 물리 서버에 걸쳐 컨테이너로 애플리케이션을 실행하는 것을 말한다.
직접 컨테이너를 관리하는 것과는 달리 클러스터에 컨테이너 관리를 위임한다.

# 12장 컨테이너 오케스트레이션: 도커 스웜과 쿠버네티스
대량의 트래픽 처리, 고가용성을 얻기 위해 여러 대의 도커 호스트로 구성된 운영 환경에서 앱 실행 방법을 알아보자.
여전히 컨테이너를 통해 앱을 실행하지만, 여러 대의 도커 호스트와 컨테이너를 관리해주는 레이어가 추가된다.

이 관리 레이어를 오케스트레이션이라고 한다. 
주요 도구로는 도커 스웜과 쿠버네티스가 있다.
도커 스웜은 도커에 내장된 형태로, 강력한 컨테이너 오케스트레이션 도구다.
쿠버는 나중에 알아서 알아보자.

최종 목표가 쿠버네티스를 익히는 것이라도 스웜을 먼저 익히는 것이 좋다.
쿠버는 초기 학습 과정이 어렵다. 스웜은 비교적 수월하다.

## 12.1 컨테이너 오케스트레이션 도구란?
도커 컴포즈는 단일 도커 호스트에서 컨테이너 실행을 위한 도구다.
단일 호스트로만 구성되지 않는 경우가 많은데,
단일 호스트의 고장은 전체 애플리케이션 중단을 일으킨다.
서비스의 고가용성을 위해 오케스트레이션이 필요하다.

오케스트레이션 도구란 클러스터를 구성하는 여러 대의 호스트 컴퓨터를 의미한다.
컨테이너들을 관리하고, 작업을 여러 컴퓨터에 분배하고 네트워크 트래픽 부하를 고르게 분산시키고, 상태 불량 컨테이너를 재시작하거나 새 컨테이너로 교체한다.

각 호스트에 도커 설치 후 클러스터를 만들고 오케스트레이션 플랫폼(스웜, 쿠버네티스, ...)에 등록 후 관리한다.

컨테이너 플랫폼은 여러 대의 서버를 묶어 관리한다.
클러스터 API를 통해 yaml을 전달하면, 앱이 배포 되고 오케스트레이션 도구가 어떤 서버에서 어떤 컨테이너를 동작할지 결정한다.

컨테이너는 오케스트레이션 도구에 의해 관리되고, 컨테이너의 건강상태를 보고 한다.
클러스터는 내부에 배포된 앱들에 대한 모든 정보가 담긴 데이터와, 스케줄러, 호스트간 통신 시스템 등이 있다.
일부 컨테이너가 동자하지 않는다면 상태 이상을 체크하여 재실행하거나 새 컨테이너로 대체한다.

## 12.2 도커 스웜으로 클러스터 만들기
도커 스웜은 도커 엔진에 포함돼 있다.
도커 엔진을 스웜 모드로 전환해 클러스트를 초기화하면 된다.

실습) p.345 참고
- docker swarm init
  - 도커 스웜 초기화
  - 클러스터 매니저 - 해당 컴퓨터는 스웜 매니저가 된다.
    - 매니저, 워커 두 가지 역할 중 하나를 맡고, 다른 컴퓨터를 워커로 참여시키는 명령어도 있다.
      - docker swarm join --token SWMTKN-1-3-21kdfsdkfjsdfslvlnfvld-fdvdfvv 192.168.65.3:2377

매니저는 클러스터 관리 작업 직접 수행한다.
클러스터 데이터베이스, API, 컨테이너 모니터링, 스케줄링 모두 매니저 노드에 저장되고 동작한다.
워커는 매니저의 스케줄링에 따라 컨테이너 실행, 상태를 주기적으로 매니저에 보고한다.
물론 매니저도 워커 역할 수행 가능하다.

스웜에 추가된 컴퓨터를 노드라고 부른다.
스웜에 노드로 추가 하려면, 스웜과 같은 네트워크에 있어야 하고, PW 역할을 하는 참가 토큰을 매니저로부터 발급 받아야 한다.
- 같은 네트워크
- 매니저에게 참가 토큰 발급 받기

실습) p.346 참고
- docker swarm join-token worker
  - 워커 참가 토큰
- docker swarm join-token manager
  - 매니저 참가 토큰
- docker node ls

단일 노드나 여러 노드나 동작 방식은 같지만, 
단일 노드는 여러 노드에 비해 높은 고가용성을 가질 순 없고 컨테이너 수를 증가시키는 스케일링이 불가능하다.

개발, 테스트 환경은 단일 노드 스웜으로 충분하다.
동작 방식은 노드가 여러 개 스웜과 동일하다.

운영용 스웜은 세 개의 매니저 노드가 있다.
클러스터 데이터베이스, 스케줄러, 모니터링 등 가용성이 향상된다.
세 개의 매니저 노드로 수백 대의 워커 노드 관리 가능하고,
애플리케이션을 대규모로 스케일링 가능하다.

쿠버네티스보다 도커 스웜은 클러스터 구성, 관리 작업이 단순하다.
- docker swarm init
- docker swarm join

## 12.3 도커 스웜 서비스로 애플리케이션 실행하기
컨테이너 플랫폼은 컨테이너를 직접 실행할 필요 없이 대신 실행해준다.
서비스는 컨테이너를 추상화한 개념이다.
하나의 서비스가 여러 컨테이너로 배포 될 수 있다는 점에서 도커 컴포즈의 서비스와 의미가 같다.

서비스는 컨테이너와 동일한 정보로 정의되고, 서비스 이름이 도커 네트워크상 도메인이 된다는 점도 컨테이너와 같다.
차이점이라면, 여러 개의 레플리카를 가질 수 있다는 점이다.

실습) p.419 참고
- docker service create -name timecheck --replicas 1 diamol/ch12-timecheck:1.0
- docker service ls
  - 레플리카 수 등 정보 출력

서비스는 도커 스웜의 일급 객체다.
서비스를 다루려면 도커 엔진이 스웜 모드이거나, 스웜 매니저에 연결된 상태여야 한다.
실습에서 실행한 서비슨느 하나의 레플리카를 실행중이다.

레플리카는 평범한 도커 컨테이너다.
노드가 하나 뿐인 스웜에서 모든 레플리카가 같은 서버에서 실행된다.

컨테이너 관리를 스웜이 대신하여 컨테이너를 직접 다룰 일이 적어진다. 물론 직접 다룰 수도 있다.
실습) p.420 참고
- docker service ps timecheck
- docker container rm -f $(docker container ls --last 1 -q)
- docker service ps timecheck 
  - 컨테이너 삭제했지만, 여전히 스웜이 실행중이다. 
  - 수동 컨테이너 삭제 -> 스웜은 컨테이너 부족으로 판단, 대체 컨테이너 실행(새 레플리카)
    - 즉 컨테이너들을 서비스로 보고 스웜에 관리를 맡긴다. 수동 삭제해도, 스웜 판단에 의해 새 컨테이너가 생성된다.

실습) p.423 참고
- docker service logs --since 10s timecheck
  - 최근 10초간 로그 출력
- docker service inspect timecheck -f '{{.Spec.TaskTemplate.ContainerSpec.Image}}'
  - 서비스 정보 중 이미지 정보 출력

도커 컴포즈와 스웜의 가장 큰 차이는 앱 정의 저장 공간의 존재 유무다.
앱 정의는 컴포즈 파일에만 들어있다.
스웜 모드는 앱 정의가 클러스터에 정의되므로, yaml 파일이 존재하지 않아도 원격에서 앱 관리 가능하다.

실습) p.503 참고
- docker service update --image diamol/ch12-timecheck:2.0 timecheck
  - 서비스에 사용된 이미지 버전 수정
- docker service ps timecheck
  - 서비스의 레플리카 목록 확인
- docker service logs --since 20s timecheck
  - 레플리카 로그 확인

모든 컨테이너 오케스트레이션 도구는 앱 중단을 하지 않고 점진적으로 컨테이너를 교체해 나가는 롤링 업데이트 방식을 사용한다.
애플리케이션을 여러 개의 레플리카로 실행했다면 중단없이 업데이트 가능하다.
롤링 업데이트는 세밀한 설정이 가능하다. 
롤링 업데이트는 구버전, 신버전 모두 실행되고 있으므로, 직접 관리가 필요하게 된다.

자동화된 롤링 업데이트는 수동 배포에 비하여 크게 발전한 방식이다.
업데이트 과정에서 신규 투입 컨테이너 상태를 확인하는데, 상태 이상시 자동 업데이트를 중단하여 앱의 문제가 발생하지 않도록 한다.
스웜은 이전 버전 서비스 정의 내용이 남아 있으므로, 명령 한 번으로 이전 버전으로 롤백 할 수 있다.

실습) p.508 참고
- docker service update --rollback timecheck
- docker service ps timecheck
- docker service logs --since 25s timecheck

롤백 과정도 롤링을 거친다.
되돌아가는 것이기 떄문에 이미지 태그를 지정할 필요 없다.
이전 상태의 정의를 기억하지 않아도 되는 것이 장점이다.

스웜 모드에서는 컨테이너보단 서비스를 주로 다룬다.
스웜 모드에서 컨테이너간 도커 네트워크를 통해 통신되고, 외부 트래픽은 공개된 포트로만 컨테이너로 전달된다.

## 12.4 클러스터 환경에서 네트워크 트래픽 관리하기
스웜 모드의 네트워크는 표준 TCP/IP 방식이다.
1. 컴포넌트는 도메인 네임으로 서로를 식별한다.
2. 도커 DNS 서버는 도메인 네임을 조회해 IP 주소를 알아낸다.
3. IP 주소로 트래픽을 전달한다.
4. 트래픽은 컨테이너로 전달되고, 컨테이너가 응답을 보낸다.

스웜 모드에서는 오버레이 네트워크라는 새로운 형태의 도커 네트워크를 사용 할 수 있다.
오버레이 네트워크: 클러스터에 속한 모든 노트를 연결하는 가상 네트워크
서비스의 이름을 도메인 네임삼아 다른 서비스와 통신 가능하다.

서로 다른 오버레이 네트워크에 속한 서비스간 통신은 불가능하다.

오버레이 네트워크와 도커 네트워크의 차이점 중 하나는,
도커 컴포즈를 통해 하나의 서비스를 여러 개의 컨테이너로 스케일링 했을 때, 서비스 질의는 전적으로 서비스를 사용하는(컨슈머) 쪽이었다.
즉 서비스 질의시 포함된 모든 컨테이너의 IP 주소가 응답에 포함되고, 사용자 쪽에서 셀렉해서 질의한다.
반면 스웜에서 서비스는 수백 개의 레플리카를 가질 수 있으므로, 오버레이 네트워크에서는 서비스를 가르키는 가상 IP 주소 하나만 반환한다.

실습) p. 515
- docker service rm timecheck
- docker network create --driver overlay iotd-net
- docker service create --detach --replicas 3 --network iotd-net --name iotd diamol/ch09-image-of-the-day
  - --detach 옵션을 통해 CLI 도구가 레플리카가 모두 실행될 때 까지 기다리지 않는다.
- docker service create --detach --replicas 2 --network iotd-net --name accesslog diamol/ch09-access-log
- docker service ls

두 서비스를 오버레이 네트워크에 연결했다.
도커 네트워크에도 종류가 있다.
스웜 모드에서는 overlay가 기본 타입으로 사용된다.
명시적으로 타입 지정을 습관화하는 것이 좋다.

가상 IP 주소 확인하려면 레플리카 컨테이너에서 터미널로 접속하는 방법이 가장 간단하다.
터미널을 통해 서비스 이름으로 DNS 조회해 응답에서 IP 주소를 확인한다.
실습) p.517 참고
- docker container exec -it $(docker container ls --last 1 -q) sh
  - 가장 최근 실행 컨테이너의 터미널을 연다.
    - 어느 서비스에 속한 레플리카라도 무방
- nslookup iotd
- nslookup accesslog

서비스에 여러 개의 컨테이너를 실행함에도 서비스마다 하나의 IP 가상 주소가 조회된다.
서비스마다 모든 레플리카를 공유하는 주소다.

오버레이 네트워크는 앱 관점에서 전혀 겉으로 드러나지 않는다.

인그레스 네트워크는 스웜을 구성하는 모든 노드가 서비스가 공개한 포트를 감시하는 방식으로 동작한다.
따라서 모든 노드에 요청이 도달 될 수 있다.

실행중이지 않은 컨테이너에 도달된 경우, 다른 노드로 요청을 포워딩한다.
노드 안에 컨테이너가 여러 개라면, 도커 엔진이 고르게 요청을 분배한다.

서비스 포트 공개시 인그레스 네트워크가 기본적으로 사용된다.

실습) p.522 참고
- docker service create --detach --name image-gallery --network iotd-net --publish 8010:80 --replicas 2 diamol/ch09-image-gallery
- docker service ls

윈도 호스트 컴은 localhost를 통해 스웜 서비스에 접근 할 수 없다.
윈도 네트워크 스택이 지닌 한계 탓이다.

앱 배포와 관리에 클러스터 크기는 영향이 크지 않다.

## 12.5 도커 스웜과 쿠버네티스 중 무엇을 사용할까?
도커 스웜은 상대적으로 기능이 간단한 컨테이너 오케스트레이션 도구로 설계됐다.

네트워크, 서비스 개념을 도커 컴포즈에서 차용해, 오케스트레이션 도구로서 도커 엔진에 잘 녹였다.
오케스트레이션 도구는 여럿 있지만, 대부분 도커 스웜 또는 쿠버네티스를 사용한다.

쿠버네티스는 확장성이 뛰어나 로드밸런서, 스토리지 등 앱 배포를 지원하는 자사 프러덕트와 통합하기 유리하다.
현재 도커 스웜을 매니지드 서비스 형태로 제공하는 클라우드 사업자는 없다.
쿠버와 달리 확장성이 부족해 자사  프러덕트와 통합하기 곤란한 것도 하나의 이유가 된다.

쿠버네티스용 yaml 파일은 쿠버에서만 사용 가능한 리소스가 기술되고, 스웜에 비해 훨씬 어렵고 복잡하다.
yaml 정의도 스웜에 비해 쿠버는 5~10배 더 길어진다.

따라서 도커 스웜을 먼저 도입 한 후 없는 기능이 필요해질 때, 쿠버네티스로 이전 방식을 추천한다.
이전시 낭비 비용이 없다.
쿠버네티스 이전 결정 기준 사항
1. 인프라스트럭처 
앱을 클라우드 환경에 배포시 쿠버가 더 적합하다.
온프레미스 환경이라면 관리 면에서 스웜이 훨씬 간편하다.

2. 학습 곡선
스웜은 도커와 도커 컴포즈의 연장선상에 있어 다소 부담이 덜 하다.
쿠버를 위해 새로운 도구를 학습해야 한다.

3. 기능
쿠버는 복잡하다. 세세한 설정 기능이 많기 때문이다.
예를 들어 블루 그린 배포, 자동 스케일링, 역할 기반 접근 제어 등 쉽게 적용 가능하다.
반면 스웜에서는 까다롭다.

4. 미래를 위한 투자
오픈 소스 커뮤니티는 매우 활동적이다.
스웜은 쿠버와 달리 신규 기능이 추가된지 꽤 됐다.  

결국 기술 로드맵의 종착점은 쿠버네티스가 될 것이다.
하지만 서두룰 필요는 없다.


## 12.6 연습 문제
- 실습) p.532 참고
- docker network --driver overlay test-12
- docker service create --detach --replicas 3 --network test-12 --name num-api diamol/numbers-api:v3
- docker service create --detach --replicas 3 --network test-12 --publish 8020:80 --name num-web diamol/numbers-web:v3
---

# 13장 도커 스웜 스택으로 분산 애플리케이션 배포하기
12장까지 해오면서 CLI를 통해 꽤 많은 시간을 할애했다.
그러나 실무에서는 명령행 도구를 사용할 일이 없을 것이다.
실무에서는 앱을 yaml 파일로 정의해 매니저 노드에 이 파일을 전달하는 방법을 쓴다.
이를 통해 오케스트레이션 도구가 실행 방법을 결정한다.

## 13.1 도커 컴포즈를 사용한 운영 환경
도커 스웜은 컴포즈를 만날 때 위력을 발휘한다.
개발 환경을 동일한 파일 포맷을 사용할 수 있는 덕분이다.

```yaml
version: "3.7"

services:
  todo-web:
    image: diamol/ch06-todo-list
    ports: 
      - 8080:80
```
스웜에도 동일한 파일을 사용해 앱 배포 가능하다.
마찬가지로 배포 후 레플리카 하나를 실행 중인 서비스가 생성되고, 이 서비스는 인그레스 네트워크를 통해 포트를 공개하고 있을 것이다.
스웜 모드에서는 앱 배포시 스택을 만든다.
스택은 서비스, 네트워크, 볼륨 등 여러 개의 도커 리소스를 묶어 만든다.

실습) p.536 
- cd ./ch13/exercises
- docker stack deploy -c ./todo-list/v1.yml todo
  - 컴포즈 파일로 스택을 배포한다.
  - 도커 스웜이 yaml 파일을 통해 앱의 원하는 상태 파악 후 리소스 생성
  - 스택은 스웜 리소스다.
- docker stack ls
- docker service ls

별도 추가 설정 없이 컴포즈 파일만으로 앱을 배포할 수 있다.
스웜에 노드가 두 개 이상이면 고가용성 확보 가능하다.
스웜 모드의 추가 기능이 몇 개 있는데, 그 중 deploy 항목이 있다.
deploy 프로퍼티는 클러스터에서 앱을 실행해야 의미가 있다.
단일 서버에서 실행시 해당 프로퍼티는 무시된다.

```yaml
services:
  todo-web:
    image: diamol/ch06-todo-list
    ports: 
      - 8080:80
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: "0.50"
          memory: 100M
```

레플리카가 여러 개 실행되면 앱의 처리 용량도 그만큼 늘어나고, 하나의 앱이 고장나도 회복이 가능하다.
자원에 대한 상한 설정을 통해 자원을 고갈시키는 것을 방지할 수 있다.
하나의 레플리카가 CPU 코어의 경우 코어 한 개의 50%, 메모리의 경우 100MB까지 점유하도록 설정

실습) p. 541
- docker stack deploy -c ./todo-list/v2.yml todo
- docker service ps todo_todo-web

실습) p.543 참고
- docker stack services todo
- docker stack ps todo
- docker stack rm todo

## 13.2 컨피그 객체를 이용한 설정값 관리
클러스터에 저장되는 도커 컨피그 객체를 이용해 운영 환경에서 앱 설정 값을 제공 할 수 있다.

실습) p.548 참고
- docker config create todo-list-config ./todo-list/configs/config.json
  - 로컬 파일에 위치한 json 파일로 컨피그 객체를 만든다.
- docker config ls

컨피그 객체는 민감한 데이터를 보관하기 위한 수단이 아니다.
암호화되지 않는 파일이다.
클러스터에 접근 권한이 있다면 누구든지 전체 내용을 확인 가능하다.
- 민감한 데이터 목록
  - 데이터베이스 접속 정보
  - 운영 환경용 API 키
  - API URL
  - ...
민감 정보는 도커 스웜에 비밀 값 리소스에 보관하자.

실습) p.550 참고
- docker config inspect --pretty todo-list-config

```yaml
# 컨피그 객체는 컨테이너 파일 시스템을 통해 서비스에 전달된다.
services:
  todo-web:
    image: diamol/ch06-todo-list
    ports:
      - 8080:80
    configs:
      - source: todo-list-config
        target: /app/config/config.json
        
# ...

configs:
  todo-list-config:
    external: true
```

실습) p.553
- docker stack deploy -c ./todo-list/v3.yml todo
- docker stack services todo

## 13.3 비밀 값을 이용한 대외비 설정 정보 관리하기
비밀 값은 컨피그 객체와 유사한 점이 많다.
1. 로컬 파일로 생성
2. 클러스터 데이터베이스에 저장
3. 서비스 정의에서 비밀 값 참조
4. 파일 시스템을 통해 컨테이너에 비밀 값 전달

비밀 값은 항상 암호화된 상태로 존재하다가, 컨테이너에 전달된 상태에서만 복호화 가능하다.

실습) p.556 참고
- docker secret create todo-list-secret ./todo-list/secrets/secrets.json
  - 로컬에 위치한 json 파일 내용으로 비밀 값 생성
- docker secret inspect --pretty todo-list-secret
  - 사용 방법은 컨피그 객체와 동일하다.
  - 그러나 비밀 값 내용 확인 불가능

```yaml
# 비밀 값과 컨피그 객체 사용해 앱 설정
services:
  todo-web:
    image: diamol/ch06-todo-list
    ports:
      - 8080:80
    configs:
      - sources: todo-list-config
        target: /app/config/config.json
    secrets:
      - sources: todo-list-secret
        target: /app/config/secrets.json
        
#...
secrets:
  todo-list-secret:
    external: true
```

실습) p.559 참조
- docker stack deploy -c ./todo-list/v4.yml todo
- docker stack ps todo

컨피그 객체와 비밀 값은 수정 불가다.
한 번 생성 후 내용은 변경되지 않는다.
변경이 필요하면 새로운 컨피그 객체나 비밀 값을 만들어야 한다.
- 세 단계를 거친다.
  1. 변경 내용을 담은 객체를 기존과 다른 이름으로 만든다.
  2. 컴포즈 파일에 새로 생성한 컨피그 객체 또는 비밀 값으로 변경
  3. 변경된 컴포즈 파일로 스택을 배포

결국 설정 값 수정시 서비스 업데이트가 필요하다.
즉 컨테이너도 새 것으로 교체된다.

스웜과 달리 쿠버에서는 수정 가능하다. 그러나 결국 컨테이너 교체가 되는 경우도 생긴다.

서비스 업데이트를 너무 두렵게 생각하지마라.
롤링 업데이트로 교체 하고, 도커 한 달에 한 번을 주기로 서비스는 업데이트 해야 될 것이다.

## 13.4 스웜에서 볼륨 사용하기
볼륨은 컨테이너와 별개의 생애주기를 갖는 스토리지의 단위다.
마치 컨테이너 파일 시스템의 일부처럼 사용 가능하지만, 외부에 존재하는 리소스다.

오케스트레이션 플랫폼에서도 볼륨의 개념은 같다.
하지만 데이터 저장 방식에 큰 차이가 있다.

레플리카를 대체하는 새 레플리카는 각각 다른 노드에서 실행되는 경우 로컬 볼륨에 접근할 수 없다.
이는 서비스가 데이터가 있는 특정 노드에서만 실행되게끔 고정하면 된다.

실습) p. 566 참고
- docker node update --label-add storage=raid $(docker node ls -q)

```yaml
# 볼륨 제약 사항
services:
  todo-db:
    image: diamol/postgres:11.5
    volumes:
      - todo-db-data:/var/lib/postgresql/data
    deploy:
      placement:
        constraints:
          - node.labels.storage == raid
            
#...
volumes: 
  todo-db-data:
```
- 스웜의 기본 볼륨 드라이버를 사용해 로컬 디스크 볼륨 생성한다.
- 데이터베이스 레플리카는 스토리지 레이블 일치 노드에서만 실행된다.
- 이 노드는 todo-db-data 로컬 볼륨을 생성해 데이터가 저장된다.

실습) p.567 참고
- docker volume ls -q
- docker stack deploy -c ./todo-list/v5.yml todo
- docker volume ls -q

실습) p.570 참고
- docker stack deploy -c ./todo-list/v6.yml todo
- docker stack ps todo
- docker volume ls -q

## 13.5 클러스터는 스택을 어떻게 관리하는가?
잘 기억해둬야 할 것들
1. 스웜도 볼륨 생성, 삭제 가능하다. 기본 볼륨 사용시 스택 제거시 함께 삭제되지만, 네이밍이 주어진 볼륨 사용시 볼륨은 삭제되지 않는다.
2. 비밀 값, 컨피그 객체는 설정 값이 든 파일을 통해 클러스터에 업로드하는 방법으로 생성하고, 수정 불가다.
3. 네트워크는 앱과 별도로 관리된다. 
4. 서비스는 스택 배포될 때 생성되거나 제거된다. 

## 13.6 연습 문제
- p. 577 참고
```yaml 
# 땡
version: "3.7"

services:
  accesslog:
    image: diamol/ch09-access-log
    deploy:
      replicas: 3
  nasa-api:
    image: diamol/ch09-image-gallery
    ports:
      - 8030:80
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: "0.50"
          memory: "100M"
```

```yaml
version: "3.7"

services:
  accesslog:
    image: diamol/ch09-access-log
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: "0.50"
          memory: 100M
    networks:
      - app-net

  iotd:
    image: diamol/ch09-image-of-the-day
    ports:
      - 8088:80
    deploy:
      replicas: 5
      resources:
        limits:
          cpus: "1"
          memory: 200M
    networks:
      - app-net

  image-gallery:
    image: diamol/ch09-image-gallery
    ports:
      - 80:80
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: "0.75"
          memory: 75M
    networks:
      - app-net

networks:
  app-net:
    name: image-gallery-prod

```
---

# 14장 업그레이드와 롤백을 이용한 업데이트 자동화
컨테이너를 통해 앱을 실행하고 있다면, 오케스트레이션 도구와 조합을 통해 무중단 업데이트가 가능하다.
클러스터를 통해 새 컨테이너를 추가로 실행 가능하고, 이미지에 헬스 체크를 포함시켜 클러스터가 이상 유무를 파악가능하다.

롤링 업데이트가 기본 상태에서 어떻게 동작하고, 어디까지 설정 할 수 있는지 이해하고 있어야 한다.
롤링 업데이트는 모든 오케스트레이션 도구가 비슷하다.
롤링 업데이트의 충분한 이해는 앱에 맞는 설정을 찾을 수 있다.

## 14.1 도커를 사용한 애플리케이션 업그레이드 프로세스
도커 이미지는 굉장히 단순한 패키징 포맷이다.

컨테이너 앱의 배포는 최소 네 가지 주기를 고려해야 한다.
1. 의존 모듈의 업데이트 -> 정해진 주기 없이 업데이트
2. 앱 코드 컴파일에 사용되는 SDK 업데이트
3. 앱이 동작하는 플랫폼의 업데이트
4. OS 업데이트 -> 월 단위로 충분

즉 컨테이너 앱 뿐만 아니라, 파생되는 여러 업데이트도 신경써야 한다.
이미지에 포함되는 SDK, 런타임, OS 이미지 등 최신 보안을 유지하기 위한 업데이트도 최신을 유지해줘야 한다.
이미지는 내가 만든 이미지 달랑 하나가 아니라, 이미지 안에 여러 이미지가 포함될 수 있다.

앱의 변경 없이 업데이트를 지속해야한다는 것은 부담스러운 일이다.
배포 절차가 까다로울수록 부담은 가중된다.

하지만 업데이트 배포는 평범한 업무로 사람의 개입 없이 진행되게 만들어 신뢰감, 배포 주기를 기다리는 일이 없도록 포함 시킬 수 있다.
빌드에 대한 신뢰는 성공적인 배포가 지속돼야 한다.
이 과정의 핵심은 앱 헬스 체크다.
헬스 체크를 통해 안전한 업데이트와 롤백을 한다.

헬스 체크와 오버라이드 컴포즈 파일을 활용해보자.
여러 개로 나뉜 컴포즈 파일은 스택 배포에 사용 할 수 없다.
따라서 오버라이드를 활용해 하나의 컴포즈 파일로 병합하여 도커 스웜 스택에 배포한다. 

실습) p.583 참고
- cd ch14/exercises
- docker-compose -f ./numbers/docker-compose.yml -f ./numbers/prod.yml config > stack.yml
- docker stack deploy -c stack.yml numbers
- docker stack services numbers
  - api는 모드가 레플리카인데, web은 global로 되어 있다.
  - global로 동작하는 서비스는 한 노드에 레플리카 하나만 실행한다
    - 인그레스 네트워크 우회 목적으로 사용, 리버스 프록시 상황에서 유용하게 사용 가능한 모드다.

```yaml
# global 모드 서비스는 인그레스 네트워크 대신 호스트 네트워크 사용 가능
services:
  numbers-web:
    ports:
      - target: 80
        published: 80
        mode: host
    deploy:
      mode: global
```
- mode: global -> 해당 서비스는 한 노드에서 한 개의 컨테이너만 실행
- mode: host -> ports 항목에 추가시 서비스를 인그레스 네트워크 대신 호스트의 80 포트와 연결한다.
  - 한 노드에 레플리카 하나로 무방한 가벼운 앱이거나, 네트워크 성능이 매우 중요해서 인그레스 네트워크 내 라우팅에 따른 오버헤드 제거 필요시 유용한 설정 패턴이다.

실습) p.588 참고
- docker-compose -f ./numbers/docker-compose.yml -f ./numbers/prod.yml -f ./numbers/prod-healthcheck.yml -f ./numbers/v2.yml --log-level ERROR config > stack.yml
- docker stack deploy -c stack.yml numbers
  - 스택 업데이트
- docker stack ps numbers

도커 스웜의 롤링 업데이트는 섬세하게 결정된 기본 값을 따라 동작한다.
레플리카는 하나씩 교체되고, 새 컨테이너 정상 실행 확인 후 다음 컨테이너 업데이트에 들어간다.
롤링 업데이트는 새 컨테이너 실행 전 기존 컨테이너를 종료한다.
만약 새 컨테이너가 정상 시작되지 않는다면 전체 업데이트가 중단된다.

그런데, 왜 새 컨테이너 실행 전 기존 컨테이너를 종료할까? 앱이 중지된 조금의 텀은?
새 컨테이너가 제대로 동작한다는 보장이 없는데도 왜 그럴까?
그리고 롤링 업데이트 실패시, 반쯤 망가져있을 수도 있는 시스템을 그대로 두고 왜 업데이트를 중단할까?

다행히 롤링 업데이트는 이러한 동작 제어가 가능한 세세한 설정 옵션을 제공한다.

## 14.2 운영 환경을 위한 롤링 업데이트 설정하기
헬스 체크 기능을 통해 자기 수복성을 갖는 애플리케이션을 만들 수 있다.
실제 앱의 컨테이너에 간헐적인 이상 발생으로 클러스터가 컨테이너를 모니터링을 통해 헬스 체크 결과를 기반으로 
컨테이너를 교체하고 앱 상태를 정상으로 유지하는 것은 흔한 일이다.

롤링 업데이트 세세한 설정 옵션 중 "deploy: update_config" 항목을 알아보자.
```yaml
# 롤링 업데이트 커스텀 설정 예
services:
  numbers-api:
    deploy:
      update_config:
        parallelism: 3
        monitor: 60s
        failure_action: rollback
        order: start-first
```
- update_config: 네 가지 프로퍼티를 통해 롤링 업데이트 과정 커스텀 설정 가능하다.
  1. parallelism: 한 번에 교체할 레플리카 수, 기본 값은 1이다. 롤링 업데이트를 빠르고 이상을 더 잘 발견하도록 3으로 설정
  2. monitor: 다음 컨테이너 교체 전 새 컨테이너 이상 여부 모니터링하는 시간, 기본 값은 0이다. 시간을 늘리면 롤링 업데이트 신뢰성이 증가한다.
  3. failure_action: 설정 시간 내 헬스 체크 실패 또는 롤링 업데이트 실패시 조치, 기본 값은 중지이지만 이전 버전으로 롤백 가능하다.
  4. order: 레플리카 교체 절차 순서, stop-first가 기본 값으로

상황에 따라 롤링 업데이트 설정 옵션의 조정이 필요 할 수 있지만, parallelism는 레플리 카 수의 30% 정도로 설정하면 꽤 빠르게 진행 가능하다.
레플리카 교체가 완전히 끝난 후 다음 레플리카 교체가 가능하도록 설정하자.

실습) p.594 참고
- docker-compose -f ./numbers/docker-compose.yml -f ./numbers/prod.yml -f ./numbers/prod-healthcheck.yml -f ./numbers/prod-update-config.yml -f ./numbers/v3.yml --log-level ERROR config > stack.yml
- docker stack deploy -c stack.yml numbers
- docker stack ps numbers

스웜의 서비스 정보를 좀 더 쉽게 구분하는 팁이 있다.
inspect 명령에 pretty 플래그를 적용하면 된다.

실습) p.597 참고
- docker service inspect --pretty numbers_numbers-api

업데이트 설정 변경시 꼭 알아둬야 할 점은, 이후 배포에도 이들 설정을 포함시켜야 한다.
즉, 동일하게 오버라이드 파일들을 설정해야 한다.

롤링 업데이트 설정을, 롤백시에도 동일하게 적용 가능하다.
운영 환경에서 앱의 업데이트와 롤백 프로세스를 검증하는 것은 매우 중요하다.

## 14.3 서비스 롤백 설정하기
애플리케이션 롤백은 서비스 단위로 이루어진다.
작업이 매우 꼬이지 않은 이상 수동 롤백 명령을 할 필요는 없다.
보통 롤백은 자동화 롤링 업데이트 과정에서 신규 레플리카 모니터링 중 오류 발생으로 수행된다.
잘 설정되었다면, 업데이트 종료 후 신규 기능이 나타나지 않은 것을 깨닫게 될 뿐이다.

앱 배포는 서비스 중단 시간의 주요 원인이다.
모든 업데이트 과정이 자동화 돼 있더라도, 자동화 스크립트와 앱 정의를 작성하는 것은 여전히 사람이다.

실습) p.600 참고
- docker-compose -f ./numbers/docker-compose.yml -f ./numbers/prod.yml -f ./numbers/prod-healthcheck.yml -f ./numbers/prod-update-config.yml -f ./numbers/v5-bad.yml config > stack.yml
  - 코어 컴포즈 파일 여러 개 오버라이드 파일을 병합
- docker stack deploy -c stack.yml numbers
- docker service inspect --pretty numbers_numbers-api

위 예시는 실패 사례이다.
업데이트 실패로 롤백되지만, 사용자 입장에서는 바로 캐치할 수 없다.
실상 API 서비스 처리 용량이 50% 감소했음에도 불구하고 말이다.
실제로 동작을 하면서 이상을 꺠달아야 한다.
여러 개의 컴포즈 파일 병합시 안정성이 떨어진다.
오버라이드 파일을 분리하여 병합한다고 하더라도, 안정성이 떨어질 여지가 생긴다.

```yaml
# 업데이트 실패 시 최대한 빨리 이전 버전으로 돌아가는 롤백 설정
services:
  numbers-api:
    deploy:
      rollback_config:
        parallelism: 6
        monitor: 0s
        failure_action: continue
        order: start-first
```
- 이 설정의 목적은 앱을 가능한 빨리 이전 버전으로 롤백한다.
  - start-first를 통해 기존 레플리카 종료를 신경 쓰지 않고, 먼저 새 레플리카를 실행하도록 한다.
  - 롤백 실패도 다음 레플리카로 교체할 것이므로, 모니터링 시간 불필요

실습) p.604 
- docker-compose -f ./numbers/docker-compose.yml -f ./numbers/prod.yml -f ./numbers/prod-healthcheck.yml -f ./numbers/prod-update-config.yml -f ./numbers/prod-rollback-config.yml -f ./numbers/v5-bad.yml config > stack.yml
- docker stack deploy -c stack.yml numbers
- docker service inspect --pretty numbers_numbers-api

롤백 설정마저 이 전 설정으로 돌아간다.
오버라이드 파일이 많으면 위험이 생길 수 있다.
업데이트마다 파일을 빠짐없이 모두 정확한 순서대로 지정해야 하기 때문이다.

실습) 14.3 
- docker-compose -f ./numbers/docker-compose.yml -f ./numbers/prod-full.yml -f ./numbers/v5.yml --log-level ERROR config > stack.yml
- docker stack deploy -c stack.yml numbers
- docker service inspect --pretty numbers_numbers-api

앱이 잘 동작하고 헬스 체크도 잘 적용했다면, 버그가 일어난다 해도 새 레플리카로 교체되며 앱을 그대로 사용할 수 있다.

## 14.4 클러스터의 중단 시간
컨테이너 오케스틀이션 도구는 여러 대의 컴퓨터를 묶어 하나의 강력한 클러스터로 만든다.
각 컴퓨터는 디스크, 네트워크, 전원 등에 이상으로 중단 시간이 발생할 수 있다.
당연하게도 클러스터 규모가 클수록 이슈 발생 빈도는 잦아진다.
일부는 적극적인 조치가 필요한 경우도 있는데, 조치를 사전에 계획해둔다면 클러스터 문제를 비켜가는 데 도움 된다.

이번 장 실습은 둘 이상의 노드를 갖춘 스웜이 필요하다.
[온라인 실습 환경 (Play with Docker) 웹 사이트](https://labs.play-with-docker.com/)가 편리하다.
- 도커 안의 도커에서 실행된 컨테이너로, 마음대로 사용하고 종료하면 모두 사라진다.

실습) 14.4 PWD 사이트 인스턴스 추가 후 명령어
- ip=$(hostname -i)
- docker swarm init --advertise-addr $ip
- docker swarm join-token manager
- docker swarm join-token worker
- 노드 5개 생성 후 2, 3은 1의 매니저로, 4, 5는 1의 워커로 설정

가정)
- 노드 중 한 대가 OS 업데이트 혹은 인프라 작업 등으로 인해 사용 불가
- 이 노드에 실행 중 컨테이너가 있을 수 있어서, 안전하게 종료 후 다른 노드 실행 컨테이너로 교체
- 이 노드를 유지 보수 모드로 전환 후 재부팅 주기 도래 전 실행 X
  - 스웜에서는 유지 보수 모드를 드레인 모드라 한다. 매니저, 워커 노드 모두 드레인 모두 설정 가능
    - 그러나 두 타입의 드레인 모드는 약간의 차이가 있다.
    - 

실습)
- docker node update --availability drain node5
- docker node update --availability drain node3

드레인 모드로 설정된 노드는 실행중이던 모든 컨테이너 종료 후 새로운 컨테이너를 실행하지 않는다.

매니저 노드는 고가용성 확보를 위해 두 개 이상의 노드로 설정해야 한다.
리더 매니저 고장시 리더 자리를 이어받아야 하기 때문이다.
매니저 노드의 수는 항상 홀수를 유지해야 한다.
보통 소규모 클러스터는 세 개, 대규모 클러스터는 다섯 개의 매니저 노드를 둔다.
매니저 노드 고장 등의 사유로 짝수가 된 경우 워커 노드 중 하나를 매니저 노드로 승격시킬 수 있다.

실습) 14.4 
- docker swarm leave --force
  - 터미널을 통해 node1을 스웜에서 이탈시킨다.
- docker node update --availability active node5
- docker node promote node5
- docker node ls

도커에서 노드 제거 방식은 두 가지다.
1. 매니저 노드에서 node rm 명령어 사용
2. 해당 노드 터미널에 swarm leave 명령어 사용

도커 스웜 몇 가지 시나리오
1. 모든 매니저가 고장을 일으킨 경우
워커 노드만 남은 경우 앱은 그대로 잘 실행된다.
그러나 모니터링 주체가 없기 때문에 컨테이너 이상시 교체되지 않는다.

2. (리더가 아닌) 한 대를 제외한 모든 매니저 노드가 고장을 일으킨 경우
남은 매니저 노드도 리더가 아니라면 클러스터 통제권 상실 가능성이 있다.
리더 매니저 승계시 매니저 노드끼리 투표를 하는데, 다른 매니저 노드가 없으니 리더 매니저 승계 불가능

3. 노드 간 레플리카를 고르게 재배치하기
클러스터에 노드 추가시 자동으로 재배치되지 않는다.
service update --force 명령으로 변경 없이 강제로 서비스 업데이트시, 노드마다 고르게 레플리카를 재배치 할 수 있다.

## 14.5 스웜 클러스터의 고가용성
여러 지역의 데이터센터에 걸쳐 하나의 클러스터를 구성하는 방법으로 고가용성을 확보하는 경우가 생각보다 많다.
이러한 방법을 통해 고가용성 확보하는 것은 이론적으로는 가능하다.
매니저 노드는 IDC A에 두고, A, B, C에 워커 노드를 배치할 수 있다.
확실해 클러스터 관리가 단순해진다.

하지만 네트워크 지연 시간 문제가 생긴다.

스웜 구성 노드는 서로 활발하게 통신을 주고 받는다.
IDC A와 B 사이에 심각한 네트워크 지연이 발생한다면?
매니저 노드는 노드가 연락 두절 됐다고 판단해, 모든 컨테이너를 C의 워커 노드로 재배치하려 하게 된다.
그리고 서로 다른 IDC에 위치한 매니저가 각 각 자신이 리더 매니저라고 판단해 클러스터가 분열 될 수 있다.

지역간 거대한 규모의 장애에도 앱이 계속 동작 할 필요가 있다면, 
클러스터를 여러 개 구성하는 것 뿐이다.
당연히 오버헤드가 발생하고, 앱이 서로 다른 클러스터로 떠다닐 우려가 있지만,
이정도 문제는 충분히 통제 가능하다.

어느 지역 전체가 장애를 겪어도 앱이 계속 동작 가능하다.
외부 DNS 서비스를 활용해 사용자 트래픽을 가장 가까운 클러스터로 라우팅 할 수 있다.

## 14.6 연습 문제
- 서적 참고

---

# 15장 보안 원격 접근 및 CI/CD를 위한 도커 설정

---

# 16장 어디서든 실행할 수 있는 도커 이미지 만들기: 리눅스, 윈도, 인텔, ARM

---