# 섹션 7. Swarm Mode를 통한 확장성 확보

## Swarm Mode Cluster 시작하기
- 도커 호스트의 리소스가 부족해진다면?
  - 해당 호스트 CPU, Memory, 디스크 용량 늘리기 -> 스케일업
    - 서버 리소스는 한계, 무작정 올릴 수만 없다. 비용 증가
  - 기존 동일 호스트 추가 -> 스케일 아웃
    - 추가 서버 리소스를 함꼐 쓰기 위한 리소스 구성 필요
- Docker Swarm
  - 스케일 아웃이 가능하도록 필요 기능 제공!
    - 여러 대 도커 호스트를 하나의 도커 호스트 처럼 사용할 수 있다.
- Swarm mode
  - Docker Swarm과는 조금 차이가 있는듯? -> 더 간단.
  - 구성
    - Manager Node
    - Worker Node
- Swarm Mode Cluster 시작
  - Manager Node에서 Swarm Cluster 시작
    - docker swarm init --advertise-addr {Manager_Node_IP}
  - Work Node
    - docker swarm join --token {token_value} {Manager_Node_IP}:2377
  - 스웜 모드 사용 필요한 포트 오픈
    - Swarm Manager -> default 2377, 추가로 7946, 4789 포트에 대해 tcp, udp 포트 각각 오픈 필요
  - Worker Node 추가 (1, 2번 노드에서 각각 수행)
    - docker swarm join --token SWMTKN-1-1q1zqsjewrpynua1ffmao9sk1vrg584o7u9er318n4oxyv0ohe-0f3a3i079118ic3h819cqsno9 {Manager_Node_IP}:2377
  - Manager Node에서 워코 노드, 매니저 노드 체크
    - docker node ls
- 실습
  - 두 대의 서버 필요 (aws ec2)
    - 인강이랑 다르게 나는 Virtual Machine 두 대의 우분투 리눅스 서버 사용
      - 부팅 렉으로 도저히 사용 불가
      - 도커 컨테이너에 centos7 띄워서 진행 - yum이 없음, XXXX
        - docker run --privileged --name mycentos7-1 -d -p 8021:22 -p 8081:80 -p 8001:8000 centos:7 /sbin/init
        - docker run --privileged --name mycentos7-2 -d -p 8022:22 -p 8082:80 -p 8002:8000 centos:7 /sbin/init
        - docker run --privileged --name mycentos7-3 -d -p 8023:22 -p 8083:80 -p 8003:8000 centos:7 /sbin/init
        - docker exec -it mycentos7-1 /bin/bash
          - swarm-manager
        - docker exec -it mycentos7-2 /bin/bash
          - swarm-worker1
        - docker exec -it mycentos7-3 /bin/bash
          - swarm-worker2
      - 도커 컨테이너에 ubuntu 띄워서 진행
        - docker run -it --name swarm-manager ubuntu /bin/bash
        - docker run -it --name swarm-worker1 ubuntu /bin/bash
        - docker run -it --name swarm-worker2 ubuntu /bin/bash
    - yum install -y yum-utils
      - CentOS, Amazon Linux 등에서 사용
      - apt 대체 또는 수동 설치 필요 (Ubuntu)
        - 수동 설치
          - sudo apt install -y yum
          - sudo apt install -y yum-utils
        - 대체 -> yum-utils -> apt-utils
    - 도커 설치
      - CentOS
        - yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
        - yum install docker-ce
        - systemctl start docker
        - docker --version
      - Ubuntu
        - apt-get update
        - apt install -y apt-utils
        - apt-get install apt-transport-https ca-certificates curl gnupg-agent software-properties-common
        - curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
        - add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
          - arch 체크 -> dpkg --print-architecture
        - apt-get install docker-ce docker-ce-cli containerd.io
        - systemctl status docker
  - swarm-manager
    - docker swarm init --advertise-addr 172.31.10.19
    - docker swarm init --advertise-addr ${master_node_ip}
      - 스웜 클러스터 실행
      - 워크 노드 추가 명령어 출력, 워커 노드에 명령어 그대로 입력!
      - 워커 노드 추가 후 
        - docker node ls
          - manager, worker 모두 확인 가능
  - swarm-worker
    - docker swarm join --token ~~~ (manager 노드 - 워커 노드 추가 명령어 입력)

## Swarm Mode에 service 만들기
- Custom Image 등록
  - 도커 허브에 등록
    - docker tag mysql57:0.0 magma1/mysql57:0.0
      - 태깅 작업
    - docker push magma1/mysql57:0.0
- Docker Hub 계정 생성
- 서비스 생성
  - 스웜 모드에서는 service 단위로 사용하고 핸들링한다.
    - 동일한 이미지의 컨테이너 집합!
      - 1개 이상의 컨테이너가 될 수 있다.
        - 동일한 이미지로 생성된 것이어야 한다.
    - --with-registry-auth
      - 매니저 노드에서 로그인이 되어 있으면 워커 노드에서 로그인되어 있지 않아도 도커 이미지 다운 가능 설정
  - docker run -it --name db001 -h db001 -p 3306:3306 \
    - -v /db/db001/data:/var/lib/mysql -v /db/db001/log:/var/log/mysql \
    - -v /db/db001/conf:/etc/percona-server.conf.d \
    - -e MYSQL_ROOT_PASSWORD="root" -d mysql57:0.0
  - docker service create --name db001 --hostname db001 -p 3306:3306 \
    - --mount type=bind,source=/db/db001/date,target=/var/lib/mysql \
    - --mount type=bind,source=/db/db001/log,target=/var/log/mysql \
    - --mount type=bind,source=/db/db001/conf,target=/etc/percona-server.conf.d \
    - -e MYSQL_ROOT_PASSWORD="root" --with-registry-auth \
    - magma1/mysql57:0.0
  - docker service ls
  - docker service ps db001
  - Manager Node IP로 접속 확인
    - mysql -u root -p -h {manager_node_ip}
  - Worker Node IP로 접속 확인
    - mysql -u root -p -h {worker_node_ip}
- 실습
  - docker login
  - docker tag mysql57:0.0 itchoi0429/mysql57:0.0
  - docker push itchoi0429/mysql57:0.0
  - manager/worker node 모두 디렉토리 생성
    - mkdir -p /db/db001/data /db/db001/log /db/db001/conf
    - vi /db/db001/conf/my.cnf
    - chown -R mysql:mysql /db
    - docker service create --name db001 --hostname db001 -p 3306:3306 \
      - --mount type=bind,source=/db/db001/date,target=/var/lib/mysql \
      - --mount type=bind,source=/db/db001/log,target=/var/log/mysql \
      - --mount type=bind,source=/db/db001/conf,target=/etc/percona-server.conf.d \
      - -e MYSQL_ROOT_PASSWORD="root" --with-registry-auth \
      - magma1/mysql57:0.0
    - docker service ls
    - docker service ps db001
    - mysql -u root -p -h {manager_node_ip}
      - show databases;
      - exit
    - mysql -u root -p -h {worker_node_ip}
      - show databases;
      - exit
- Swarm Mode에서 생각해야 될 부분!
  - Swarm Node 환경에서 MySQL 컨테이너가 재시작 -> 데이터 유지 방법은?
    1. 해당 컨테이너가 특정 Node에서만 실행 되도록 하는 방법(Label)
    2. NFS 스토리지를 각 Node에 mount -> 같은 디스크를 바라보도록 하는 방법(NFS)
    3. Volume plugin 사용 방법(GlusterFS)

## Label을 이용한 서비스 생성
- docker service create로 만들어진 컨테이너가 재시작 될 경우 스웜 노드에서 랜덤하게 재시작 될 수 있다.
  - 저장 데이터 손실 가능성이 생긴다.
- Label 이용 방법
  - 각 Node에 Label 등록
  - 서비스 생성시 label을 이용해 특정 Node에서만 해당 서비스 실행되도록 제한
  - 첫 번 째 Worker Node - Label 추가
    - docker node update --label-add server_name=db001 {worker_node1_hostname}
    - docker service rm db001
      - 기조 생성 서비스 삭제
    - 모든 노드에 mysql group & user 생성
      - groupadd -g 1001 mysql
      - useradd -u 1001 -r -g 1001 mysql
    - 첫 번 째 worker node -  디렉토리 생성
      - mkdir -p /db/db001/data /db/db001/log /db/db001/conf
      - vi /db/db001/conf/my.cnf
      - chown -R mysql:mysql /db
    - docker service create --name db001 --hostname db001 -p 3306:3306 \
      - --mount type=bind,source=/db/db001/date,target=/var/lib/mysql \
      - --mount type=bind,source=/db/db001/log,target=/var/log/mysql \
      - --mount type=bind,source=/db/db001/conf,target=/etc/percona-server.conf.d \
      - -e MYSQL_ROOT_PASSWORD="root" \
      - --constraint 'node.labels.server_name==db001' \
      - magma1/mysql57:0.0
    - docker service ls
    - docker service ps db001
      - 첫 번 째 워커 노드에서 실행됐는지 체크

## NFS Server를 이용한 서비스 구성
- NFS (Network File System)
- NFS 이용 방법
  - NFS용 서버 한대 필요
    - DB 디렉토리 만들고, 스웜 모드에 포함된 모든 노드에서 각각 해당 디렉토리 마운트해서 사용!
  - NFS 서버 구성
    - rpm -qa | grep nfs-utils
      - 패키지 설치 여부
    - yum install -y nfs-utils.x86_64
      - 패키지 설치
    - systemctl start nfs-server
    - vi /etc/exports
      - /db/ {Manager_Node1_Hostname} (rw,sync) {Worker_Node1_Hostname} (rw, sync) {Worker_Node2_Hostname} (rw,sync)
    - systemctl stop nfs-server
    - systemctl start nfs-server
    - nfs 서버 구성 및 사용을 위해 서버간 2049, 111 tcp/udp port open 필요!
    - groupadd -g 1001 mysql
    - useradd -u 1001 -r -g 1001 mysql
    - mkdir -p /db/db001/data /db/db001/log /db/db001/conf
    - cd /db/db001/conf
    - vi my.cnf
      - db001 my.cnf 복붙
    - chown -R mysql:mysql /db
  - NFS 디렉토리 마운트 (Swarm Node)
    - docker service rm db001
    - rm -rf /db
    - mkdir /db
    - chown -R mysql:mysql /db
    - mount -t nfs {nfs_server_ip}:/db /db
    - cd /db
    - ls -al
  - 서비스 생성 (db001, manager node)
    - docker service create --name db001 --hostname db001 \
      - --mount type=bind,source=/db/db001/data,target=/var/lib/mysql \
      - --mount type=bind,source=/db/db001/log,target=/var/log/mysql \
      - --mount type=bind,source=/db/db001/conf,target=/etc/percona-server.conf.d \
      - -p 3306:3306 \
      - -e MYSQL_ROOT_PASSWORD="root" \
      - magma1/mysql57:0.0
    - docker service ls
    - docker service ps db001
      - 어떤 노드에 생성 됐는지 체크
    - 컨테이너가 어떤 노드에 재시작되도 동일한 디렉토리를 공유해서 사용한다!

## Volume Plugin(GlusterFS)을 이용한 서비스 구성
- 도커에 포함된 기능은 아니다.
  - 플러그인 형태로 추가 설치!
    - 다양한 제품이 있다.
      - https://docs.docker.com/engine/extend/legacy_plugins/#volume-plugins
        - 일부는 유료 제품
        - 업무 사용시 -> 기능 잘 이해하고 사용해야 한다.
          - 기능적 성능적 테스트 필요
      - GlusterFS 플러그인을 사용해보자.
        - 몇 가지 다른 형태로 구성 가능
          - 스웜 모드에 포함된 각 노드에서 서로의 데이터를 노드 간 복제해서 동일 데이터를 각 노드가 갖고 있는 형태 
- Volume Plugin - GlusterFS (ec2에 추가)
  - Storage 볼륨 추가
    - lsblk
      - 해당 인스턴스에 붙어있는 볼륨 체크
    - mkfs -t xfs /dev/nvmelnl
    - mkdir /gluster/bricks/1/brick
    - mount /dev/nvmelnl /gluster/bricks/1/brick
    - vi /etc/fstab
      - /dev/nvmelnl  /gluster/bricks/1/brick   xfs   defaults,noatime  1   1
        - 서버 리스타트시 마운트 정보 사라짐 방지!
    - 각 워커 노드에 추가
      - worker node1
        - mkdir /gluster/bricks/2/brick
        - mount /dev/nvmelnl /gluster/bricks/2/brick
      - worker node1
        - mkdir /gluster/bricks/3/brick
        - mount /dev/nvmelnl /gluster/bricks/3/brick
  - GlusterFS 설치 및 세팅
    - 3대 노드 모두 동일하게 설치 (manager, worker)
    - docker plugin install --alias glusterfs trajano/glusterfs-volume-plugin --grant-all-permissions --disable
    - docker plugin set glusterfs
    - docker plugin enable glusterfs
    - yum install -y xfsprogs.x86_64
    - yum install -y attr.x86_64
    - yum install -y glusterfs.x86_64
    - yum install -y centos-release-gluster7.noarch
    - yum install -y glusterfs-server.x86_64
    - systemctl enable glusterfsd
    - systemctl start glusterd
  - 각 노드에 /etc/hosts 파일에 Node 정보 추가
    - vi /etc/hosts
    - 172.31.10.19    ip-172-31-10-19
    - 172.31.6.58     ip-172-31-6-58
    - 172.31.4.155    ip-172-31-4-155
  - GlusterFS 관련 포트 오픈
    - 24007,24008,24009,49152,111
  - Peer probe
    - gluster peer probe ip-172-31-6-58
    - gluster peer probe ip-172-31-4-155
    - gluster pool list
      - 노드가 정상으로 추가 됐는지 체크
  - Gluster Volume 생성
    - gluster volume create gfs \
      - replica 3 \
      - ip-172-31-10-19:/gluster/bricks/1/brick \
      - ip-172-31-6-58:/gluster/bricks/2/brick \
      - ip-172-31-4-155:/gluster/bricks/3/brick \
      - force
    - gluster volume start gfs
    - gluster volume set gfs auth.allow 172.31.10.19,172.31.6.58,172.31.4.155
    - mount.glusterfs localhost:/gfs /db
    - cd /db
    - mkdir -p db001 db001/data db001/log db001/conf
    - vi /db/db001/conf/my.cnf
    - chown -R mysql:mysql /db
  - 서비스 생성 (db001, manager node)
    - docker service create --name db001 --hostname db001 \
      - --mount type=bind,source=/db/db001/data,target=/var/lib/mysql \
      - --mount type=bind,source=/db/db001/log,target=/var/log/mysql \
      - --mount type=bind,source=/db/db001/conf,target=/etc/percona-server.conf.d \
      - -p 3306:3306 \
      - -e MYSQL_ROOT_PASSWORD="root" \
      - itchoi0429/mysql57:0.0
    - docker service ls
    - docker service ps db001
      - 어떤 노드에서 실행되는지 체크
    - cd /db/db001/data
    - ls -al
      - 모든 노드에 동일 데이터가 있는지 체크

## Docker Stack in Swarm mode
- Overlay Network
- 컨테이너명을 이용해 컨테이너 접속 - Bridge Network
  - 서비스명을 이용해 접속 - Overlay Network 사용!
- Docker compose 대신 Docker stack 사용
  - 별도 설치 X
- docker stack deploy -c docker-compose.yml mysql
- docker stack ls
- docker stack ps mysql
- docker service ls
- 각 노드에서 exporter 실행
  - docker service ps mysql_db001
  - docker ps --format "table {{.ID}}\t{{.Names}}\t{{.Status}}"
  - docker exec ${container_id} sh
    - /opt/exporters/node_exporter/start_node_exporter.sh
    - /opt/exporters/mysqld_exporter/start_mysqld_exporter.sh